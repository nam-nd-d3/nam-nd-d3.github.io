{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to mlops-with-ml","title":"mlops-with-ml"},{"location":"#welcome-to-mlops-with-ml","text":"","title":"Welcome to mlops-with-ml"},{"location":"getting_started/","text":"","title":"Getting started"},{"location":"app/api/","text":"construct_response ( f ) Construct a JSON response for an endpoint's results. Source code in app/api.py def construct_response ( f ): \"\"\"Construct a JSON response for an endpoint's results.\"\"\" @wraps ( f ) def wrap ( request : Request , * args , ** kwargs ): results = f ( request , * args , ** kwargs ) # Construct response response = { \"message\" : results [ \"message\" ], \"method\" : request . method , \"status-code\" : results [ \"status-code\" ], \"timestamp\" : datetime . now () . isoformat (), \"url\" : request . url . _url , } # Add data if \"data\" in results : response [ \"data\" ] = results [ \"data\" ] return response return wrap","title":"Api"},{"location":"app/api/#app.api.construct_response","text":"Construct a JSON response for an endpoint's results. Source code in app/api.py def construct_response ( f ): \"\"\"Construct a JSON response for an endpoint's results.\"\"\" @wraps ( f ) def wrap ( request : Request , * args , ** kwargs ): results = f ( request , * args , ** kwargs ) # Construct response response = { \"message\" : results [ \"message\" ], \"method\" : request . method , \"status-code\" : results [ \"status-code\" ], \"timestamp\" : datetime . now () . isoformat (), \"url\" : request . url . _url , } # Add data if \"data\" in results : response [ \"data\" ] = results [ \"data\" ] return response return wrap","title":"construct_response()"},{"location":"app/schemas/","text":"","title":"Schemas"},{"location":"configs/config/","text":"","title":"Configurations"},{"location":"mlops_with_ml/data/","text":"CNNTextDataset ( Dataset ) Create torch.utils.data.Dataset objects to use for efficiently feeding data into our models. Usage: # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size ) Source code in mlops_with_ml/data.py class CNNTextDataset ( torch . utils . data . Dataset ): \"\"\"Create `torch.utils.data.Dataset` objects to use for efficiently feeding data into our models. Usage: ```python # Create dataset X, y = data dataset = CNNTextDataset(X=X, y=y, max_filter_size=max_filter_size) # Create dataloaders dataloader = dataset.create_dataloader(batch_size=batch_size) ``` \"\"\" def __init__ ( self , X , y , max_filter_size ): self . X = X self . y = y self . max_filter_size = max_filter_size def __len__ ( self ): return len ( self . y ) def __str__ ( self ): return f \"<Dataset(N= { len ( self ) } )>\" def __getitem__ ( self , index : int ) -> List : X = self . X [ index ] y = self . y [ index ] return [ X , y ] def collate_fn ( self , batch : List ) -> Tuple : \"\"\"Processing on a batch. It's used to override the default `collate_fn` in `torch.utils.data.DataLoader`. Args: batch (List): List of inputs and outputs. Returns: Processed inputs and outputs. \"\"\" # Get inputs batch = np . array ( batch , dtype = object ) X = batch [:, 0 ] y = np . stack ( batch [:, 1 ], axis = 0 ) # Pad inputs X = pad_sequences ( sequences = X , max_seq_len = self . max_filter_size ) # Cast X = torch . LongTensor ( X . astype ( np . int32 )) y = torch . FloatTensor ( y . astype ( np . int32 )) return X , y def create_dataloader ( self , batch_size : int , shuffle : bool = False , drop_last : bool = False ) -> torch . utils . data . DataLoader : \"\"\"Create dataloaders to load batches with. Usage: ```python # Create dataset X, y = data dataset = CNNTextDataset(X=X, y=y, max_filter_size=max_filter_size) # Create dataloaders dataloader = dataset.create_dataloader(batch_size=batch_size) ``` Args: batch_size (int): Number of samples per batch. shuffle (bool, optional): Shuffle each batch. Defaults to False. drop_last (bool, optional): Drop the last batch if it's less than `batch_size`. Defaults to False. Returns: Torch dataloader to load batches with. \"\"\" return torch . utils . data . DataLoader ( dataset = self , batch_size = batch_size , collate_fn = self . collate_fn , shuffle = shuffle , drop_last = drop_last , pin_memory = True , ) collate_fn ( self , batch ) Processing on a batch. It's used to override the default collate_fn in torch.utils.data.DataLoader . Parameters: Name Type Description Default batch List List of inputs and outputs. required Returns: Type Description Tuple Processed inputs and outputs. Source code in mlops_with_ml/data.py def collate_fn ( self , batch : List ) -> Tuple : \"\"\"Processing on a batch. It's used to override the default `collate_fn` in `torch.utils.data.DataLoader`. Args: batch (List): List of inputs and outputs. Returns: Processed inputs and outputs. \"\"\" # Get inputs batch = np . array ( batch , dtype = object ) X = batch [:, 0 ] y = np . stack ( batch [:, 1 ], axis = 0 ) # Pad inputs X = pad_sequences ( sequences = X , max_seq_len = self . max_filter_size ) # Cast X = torch . LongTensor ( X . astype ( np . int32 )) y = torch . FloatTensor ( y . astype ( np . int32 )) return X , y create_dataloader ( self , batch_size , shuffle = False , drop_last = False ) Create dataloaders to load batches with. Usage: # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size ) Parameters: Name Type Description Default batch_size int Number of samples per batch. required shuffle bool Shuffle each batch. Defaults to False. False drop_last bool Drop the last batch if it's less than batch_size . Defaults to False. False Returns: Type Description DataLoader Torch dataloader to load batches with. Source code in mlops_with_ml/data.py def create_dataloader ( self , batch_size : int , shuffle : bool = False , drop_last : bool = False ) -> torch . utils . data . DataLoader : \"\"\"Create dataloaders to load batches with. Usage: ```python # Create dataset X, y = data dataset = CNNTextDataset(X=X, y=y, max_filter_size=max_filter_size) # Create dataloaders dataloader = dataset.create_dataloader(batch_size=batch_size) ``` Args: batch_size (int): Number of samples per batch. shuffle (bool, optional): Shuffle each batch. Defaults to False. drop_last (bool, optional): Drop the last batch if it's less than `batch_size`. Defaults to False. Returns: Torch dataloader to load batches with. \"\"\" return torch . utils . data . DataLoader ( dataset = self , batch_size = batch_size , collate_fn = self . collate_fn , shuffle = shuffle , drop_last = drop_last , pin_memory = True , ) LabelEncoder Encode labels into unique indices. Usage: # Encode labels label_encoder = LabelEncoder () label_encoder . fit ( labels ) y = label_encoder . encode ( labels ) Source code in mlops_with_ml/data.py class LabelEncoder : \"\"\"Encode labels into unique indices. Usage: ```python # Encode labels label_encoder = LabelEncoder() label_encoder.fit(labels) y = label_encoder.encode(labels) ``` \"\"\" def __init__ ( self , class_to_index : dict = {}): self . class_to_index = class_to_index or {} # mutable defaults ;) self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) def __len__ ( self ): return len ( self . class_to_index ) def save ( self , fp : str ): with open ( fp , \"w\" ) as fp : contents = { \"class_to_index\" : self . class_to_index } json . dump ( contents , fp , indent = 4 , sort_keys = False ) @classmethod def load ( cls , fp : str ): with open ( fp ) as fp : kwargs = json . load ( fp = fp ) return cls ( ** kwargs ) MultiClassLabelEncoder ( LabelEncoder ) Encode labels into unique indices for multi-class classification. Source code in mlops_with_ml/data.py class MultiClassLabelEncoder ( LabelEncoder ): \"\"\"Encode labels into unique indices for multi-class classification. \"\"\" def __str__ ( self ): return f \"<MultiClassLabelEncoder(num_classes= { len ( self ) } )>\" def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( y ) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of classes. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" encoded = np . zeros (( len ( y )), dtype = int ) for i , item in enumerate ( y ): encoded [ i ] = self . class_to_index [ item ] return encoded def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a collection of class indices. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): classes . append ( self . index_to_class [ item ]) return classes decode ( self , y ) Decode a collection of class indices. Parameters: Name Type Description Default y np.ndarray Labels as (multilabel) one-hot encodings required Returns: Type Description List[List[str]] List of original labels for each output. Source code in mlops_with_ml/data.py def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a collection of class indices. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): classes . append ( self . index_to_class [ item ]) return classes encode ( self , y ) Encode a collection of classes. Parameters: Name Type Description Default y pd.Series Collection of labels as a pandas Series object. required Returns: Type Description ndarray Labels as (multilabel) one-hot encodings Source code in mlops_with_ml/data.py def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of classes. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" encoded = np . zeros (( len ( y )), dtype = int ) for i , item in enumerate ( y ): encoded [ i ] = self . class_to_index [ item ] return encoded fit ( self , y ) Learn label mappings from a series of class labels. Parameters: Name Type Description Default y Sequence Collection of labels as a pandas Series object. required Source code in mlops_with_ml/data.py def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( y ) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self MultiLabelLabelEncoder ( LabelEncoder ) Encode labels into unique indices for multi-label classification. Source code in mlops_with_ml/data.py class MultiLabelLabelEncoder ( LabelEncoder ): \"\"\"Encode labels into unique indices for multi-label classification. \"\"\" def __str__ ( self ): return f \"<MultiLabelLabelEncoder(num_classes= { len ( self ) } )>\" def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( list ( itertools . chain . from_iterable ( y ))) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of labels using (multilabel) one-hot encoding. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" y_one_hot = np . zeros (( len ( y ), len ( self . class_to_index )), dtype = int ) for i , item in enumerate ( y ): for class_ in item : y_one_hot [ i ][ self . class_to_index [ class_ ]] = 1 return y_one_hot def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a (multilabel) one-hot encoding into corresponding labels. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): indices = np . where ( np . asarray ( item ) == 1 )[ 0 ] classes . append ([ self . index_to_class [ index ] for index in indices ]) return classes decode ( self , y ) Decode a (multilabel) one-hot encoding into corresponding labels. Parameters: Name Type Description Default y np.ndarray Labels as (multilabel) one-hot encodings required Returns: Type Description List[List[str]] List of original labels for each output. Source code in mlops_with_ml/data.py def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a (multilabel) one-hot encoding into corresponding labels. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): indices = np . where ( np . asarray ( item ) == 1 )[ 0 ] classes . append ([ self . index_to_class [ index ] for index in indices ]) return classes encode ( self , y ) Encode a collection of labels using (multilabel) one-hot encoding. Parameters: Name Type Description Default y pd.Series Collection of labels as a pandas Series object. required Returns: Type Description ndarray Labels as (multilabel) one-hot encodings Source code in mlops_with_ml/data.py def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of labels using (multilabel) one-hot encoding. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" y_one_hot = np . zeros (( len ( y ), len ( self . class_to_index )), dtype = int ) for i , item in enumerate ( y ): for class_ in item : y_one_hot [ i ][ self . class_to_index [ class_ ]] = 1 return y_one_hot fit ( self , y ) Learn label mappings from a series of class labels. Parameters: Name Type Description Default y Sequence Collection of labels as a pandas Series object. required Source code in mlops_with_ml/data.py def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( list ( itertools . chain . from_iterable ( y ))) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self Stemmer ( PorterStemmer ) Source code in mlops_with_ml/data.py class Stemmer ( PorterStemmer ): def stem ( self , word ): if ( self . mode == self . NLTK_EXTENSIONS and word in self . pool ): # pragma: no cover, nltk return self . pool [ word ] if ( self . mode != self . ORIGINAL_ALGORITHM and len ( word ) <= 2 ): # pragma: no cover, nltk # With this line, strings of length 1 or 2 don't go through # the stemming process, although no mention is made of this # in the published algorithm. return word stem = self . _step1a ( word ) stem = self . _step1b ( stem ) stem = self . _step1c ( stem ) stem = self . _step2 ( stem ) stem = self . _step3 ( stem ) stem = self . _step4 ( stem ) stem = self . _step5a ( stem ) stem = self . _step5b ( stem ) return stem stem ( self , word ) :param to_lowercase: if to_lowercase=True the word always lowercase Source code in mlops_with_ml/data.py def stem ( self , word ): if ( self . mode == self . NLTK_EXTENSIONS and word in self . pool ): # pragma: no cover, nltk return self . pool [ word ] if ( self . mode != self . ORIGINAL_ALGORITHM and len ( word ) <= 2 ): # pragma: no cover, nltk # With this line, strings of length 1 or 2 don't go through # the stemming process, although no mention is made of this # in the published algorithm. return word stem = self . _step1a ( word ) stem = self . _step1b ( stem ) stem = self . _step1c ( stem ) stem = self . _step2 ( stem ) stem = self . _step3 ( stem ) stem = self . _step4 ( stem ) stem = self . _step5a ( stem ) stem = self . _step5b ( stem ) return stem Tokenizer Tokenize a feature using a built vocabulary. Usage: tokenizer = Tokenizer ( char_level = char_level ) tokenizer . fit_on_texts ( texts = X ) X = np . array ( tokenizer . texts_to_sequences ( X ), dtype = object ) Source code in mlops_with_ml/data.py class Tokenizer : \"\"\"Tokenize a feature using a built vocabulary. Usage: ```python tokenizer = Tokenizer(char_level=char_level) tokenizer.fit_on_texts(texts=X) X = np.array(tokenizer.texts_to_sequences(X), dtype=object) ``` \"\"\" def __init__ ( self , char_level : bool , num_tokens : int = None , pad_token : str = \"<PAD>\" , oov_token : str = \"<UNK>\" , token_to_index : dict = None , ): self . char_level = char_level self . separator = \"\" if self . char_level else \" \" if num_tokens : num_tokens -= 2 # pad + unk tokens self . num_tokens = num_tokens self . pad_token = pad_token self . oov_token = oov_token if not token_to_index : token_to_index = { pad_token : 0 , oov_token : 1 } self . token_to_index = token_to_index self . index_to_token = { v : k for k , v in self . token_to_index . items ()} def __len__ ( self ): return len ( self . token_to_index ) def __str__ ( self ): return f \"<Tokenizer(num_tokens= { len ( self ) } )>\" def fit_on_texts ( self , texts : List ): \"\"\"Learn token mappings from a list of texts. Args: texts (List): List of texts made of tokens. \"\"\" if not self . char_level : texts = [ text . split ( \" \" ) for text in texts ] all_tokens = [ token for text in texts for token in text ] counts = Counter ( all_tokens ) . most_common ( self . num_tokens ) self . min_token_freq = counts [ - 1 ][ 1 ] for token , count in counts : index = len ( self ) self . token_to_index [ token ] = index self . index_to_token [ index ] = token return self def texts_to_sequences ( self , texts : List ) -> List [ List ]: \"\"\"Convert a list of texts to a lists of arrays of indices. Args: texts (List): List of texts to tokenize and map to indices. Returns: A list of mapped sequences (list of indices). \"\"\" sequences = [] for text in texts : if not self . char_level : text = text . split ( \" \" ) sequence = [] for token in text : sequence . append ( self . token_to_index . get ( token , self . token_to_index [ self . oov_token ]) ) sequences . append ( sequence ) return sequences def sequences_to_texts ( self , sequences : List ) -> List : \"\"\"Convert a lists of arrays of indices to a list of texts. Args: sequences (List): list of mapped tokens to convert back to text. Returns: Mapped text from index tokens. \"\"\" texts = [] for sequence in sequences : text = [] for index in sequence : text . append ( self . index_to_token . get ( index , self . oov_token )) texts . append ( self . separator . join ([ token for token in text ])) return texts def save ( self , fp : str ): with open ( fp , \"w\" ) as fp : contents = { \"char_level\" : self . char_level , \"oov_token\" : self . oov_token , \"token_to_index\" : self . token_to_index , } json . dump ( contents , fp , indent = 4 , sort_keys = False ) @classmethod def load ( cls , fp : str ): with open ( fp ) as fp : kwargs = json . load ( fp = fp ) return cls ( ** kwargs ) fit_on_texts ( self , texts ) Learn token mappings from a list of texts. Parameters: Name Type Description Default texts List List of texts made of tokens. required Source code in mlops_with_ml/data.py def fit_on_texts ( self , texts : List ): \"\"\"Learn token mappings from a list of texts. Args: texts (List): List of texts made of tokens. \"\"\" if not self . char_level : texts = [ text . split ( \" \" ) for text in texts ] all_tokens = [ token for text in texts for token in text ] counts = Counter ( all_tokens ) . most_common ( self . num_tokens ) self . min_token_freq = counts [ - 1 ][ 1 ] for token , count in counts : index = len ( self ) self . token_to_index [ token ] = index self . index_to_token [ index ] = token return self sequences_to_texts ( self , sequences ) Convert a lists of arrays of indices to a list of texts. Parameters: Name Type Description Default sequences List list of mapped tokens to convert back to text. required Returns: Type Description List Mapped text from index tokens. Source code in mlops_with_ml/data.py def sequences_to_texts ( self , sequences : List ) -> List : \"\"\"Convert a lists of arrays of indices to a list of texts. Args: sequences (List): list of mapped tokens to convert back to text. Returns: Mapped text from index tokens. \"\"\" texts = [] for sequence in sequences : text = [] for index in sequence : text . append ( self . index_to_token . get ( index , self . oov_token )) texts . append ( self . separator . join ([ token for token in text ])) return texts texts_to_sequences ( self , texts ) Convert a list of texts to a lists of arrays of indices. Parameters: Name Type Description Default texts List List of texts to tokenize and map to indices. required Returns: Type Description List[List] A list of mapped sequences (list of indices). Source code in mlops_with_ml/data.py def texts_to_sequences ( self , texts : List ) -> List [ List ]: \"\"\"Convert a list of texts to a lists of arrays of indices. Args: texts (List): List of texts to tokenize and map to indices. Returns: A list of mapped sequences (list of indices). \"\"\" sequences = [] for text in texts : if not self . char_level : text = text . split ( \" \" ) sequence = [] for token in text : sequence . append ( self . token_to_index . get ( token , self . token_to_index [ self . oov_token ]) ) sequences . append ( sequence ) return sequences compute_features ( params ) Compute features to use for training. Parameters: Name Type Description Default params Namespace Input parameters for operations. required Source code in mlops_with_ml/data.py def compute_features ( params : Namespace ) -> None : \"\"\"Compute features to use for training. Args: params (Namespace): Input parameters for operations. \"\"\" # Set up utils . set_seed ( seed = params . seed ) # Load data projects_url = \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/projects.json\" projects = utils . load_json_from_url ( url = projects_url ) df = pd . DataFrame ( projects ) # Compute features df [ \"text\" ] = df . title + \" \" + df . description df . drop ( columns = [ \"title\" , \"description\" ], inplace = True ) df = df [[ \"id\" , \"created_on\" , \"text\" , \"tags\" ]] # Save features = df . to_dict ( orient = \"records\" ) df_dict_fp = Path ( config . DATA_RAW_DIR , \"features.json\" ) utils . save_dict ( d = features , filepath = df_dict_fp ) return df , features filter_items ( items , include = [], exclude = []) Filter a list using inclusion and exclusion lists of items. Parameters: Name Type Description Default items List List of items to apply filters. required include List List of items to include. Defaults to []. [] exclude List List of items to filter out. Defaults to []. [] Returns: Type Description List Filtered list of items. Usage: # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = list ( tags_dict . keys ()), exclude = config . EXCLDUE , ) Source code in mlops_with_ml/data.py def filter_items ( items : List , include : List = [], exclude : List = []) -> List : \"\"\"Filter a list using inclusion and exclusion lists of items. Args: items (List): List of items to apply filters. include (List, optional): List of items to include. Defaults to []. exclude (List, optional): List of items to filter out. Defaults to []. Returns: Filtered list of items. Usage: ```python # Filter tags for each project df.tags = df.tags.apply( filter_items, include=list(tags_dict.keys()), exclude=config.EXCLDUE, ) ``` \"\"\" # Filter filtered = [ item for item in items if item in include and item not in exclude ] return filtered iterative_train_test_split ( X , y , train_size = 0.7 ) Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Parameters: Name Type Description Default X pd.Series Input features as a pandas Series object. required y np.ndarray One-hot encoded labels. required train_size float Proportion of data for first split. Defaults to 0.7. 0.7 Returns: Type Description Tuple Two stratified splits based on specified proportions. Source code in mlops_with_ml/data.py def iterative_train_test_split ( X : pd . Series , y : np . ndarray , train_size : float = 0.7 ) -> Tuple : \"\"\"Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Args: X (pd.Series): Input features as a pandas Series object. y (np.ndarray): One-hot encoded labels. train_size (float, optional): Proportion of data for first split. Defaults to 0.7. Returns: Two stratified splits based on specified proportions. \"\"\" stratifier = IterativeStratification ( n_splits = 2 , order = 1 , sample_distribution_per_fold = [ 1.0 - train_size , train_size , ], ) train_indices , test_indices = next ( stratifier . split ( X , y )) X_train , y_train = X [ train_indices ], y [ train_indices ] X_test , y_test = X [ test_indices ], y [ test_indices ] return X_train , X_test , y_train , y_test prepare ( df , include = [], exclude = [], min_tag_freq = 30 ) Prepare the raw data. Parameters: Name Type Description Default df pd.DataFrame Pandas DataFrame with data. required include List list of tags to include. [] exclude List list of tags to exclude. [] min_tag_freq int Minimum frequency of tags required. Defaults to 30. 30 Returns: Type Description Tuple A cleaned dataframe and dictionary of tags and counts above the frequency threshold. Source code in mlops_with_ml/data.py def prepare ( df : pd . DataFrame , include : List = [], exclude : List = [], min_tag_freq : int = 30 ) -> Tuple : \"\"\"Prepare the raw data. Args: df (pd.DataFrame): Pandas DataFrame with data. include (List): list of tags to include. exclude (List): list of tags to exclude. min_tag_freq (int, optional): Minimum frequency of tags required. Defaults to 30. Returns: A cleaned dataframe and dictionary of tags and counts above the frequency threshold. \"\"\" # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = include , exclude = exclude ) tags = Counter ( itertools . chain . from_iterable ( df . tags . values )) # Filter tags that have fewer than `min_tag_freq` occurrences tags_above_freq = Counter ( tag for tag in tags . elements () if tags [ tag ] >= min_tag_freq ) tags_below_freq = Counter ( tag for tag in tags . elements () if tags [ tag ] < min_tag_freq ) df . tags = df . tags . apply ( filter_items , include = list ( tags_above_freq . keys ())) # Remove projects with no more remaining relevant tags df = df [ df . tags . map ( len ) > 0 ] return df , tags_above_freq , tags_below_freq preprocess ( text , lower = True , stem = False , stopwords = [ 'i' , 'me' , 'my' , 'myself' , 'we' , 'our' , 'ours' , 'ourselves' , 'you' , \"you're\" , \"you've\" , \"you'll\" , \"you'd\" , 'your' , 'yours' , 'yourself' , 'yourselves' , 'he' , 'him' , 'his' , 'himself' , 'she' , \"she's\" , 'her' , 'hers' , 'herself' , 'it' , \"it's\" , 'its' , 'itself' , 'they' , 'them' , 'their' , 'theirs' , 'themselves' , 'what' , 'which' , 'who' , 'whom' , 'this' , 'that' , \"that'll\" , 'these' , 'those' , 'am' , 'is' , 'are' , 'was' , 'were' , 'be' , 'been' , 'being' , 'have' , 'has' , 'had' , 'having' , 'do' , 'does' , 'did' , 'doing' , 'a' , 'an' , 'the' , 'and' , 'but' , 'if' , 'or' , 'because' , 'as' , 'until' , 'while' , 'of' , 'at' , 'by' , 'for' , 'with' , 'about' , 'against' , 'between' , 'into' , 'through' , 'during' , 'before' , 'after' , 'above' , 'below' , 'to' , 'from' , 'up' , 'down' , 'in' , 'out' , 'on' , 'off' , 'over' , 'under' , 'again' , 'further' , 'then' , 'once' , 'here' , 'there' , 'when' , 'where' , 'why' , 'how' , 'all' , 'any' , 'both' , 'each' , 'few' , 'more' , 'most' , 'other' , 'some' , 'such' , 'no' , 'nor' , 'not' , 'only' , 'own' , 'same' , 'so' , 'than' , 'too' , 'very' , 's' , 't' , 'can' , 'will' , 'just' , 'don' , \"don't\" , 'should' , \"should've\" , 'now' , 'd' , 'll' , 'm' , 'o' , 're' , 've' , 'y' , 'ain' , 'aren' , \"aren't\" , 'couldn' , \"couldn't\" , 'didn' , \"didn't\" , 'doesn' , \"doesn't\" , 'hadn' , \"hadn't\" , 'hasn' , \"hasn't\" , 'haven' , \"haven't\" , 'isn' , \"isn't\" , 'ma' , 'mightn' , \"mightn't\" , 'mustn' , \"mustn't\" , 'needn' , \"needn't\" , 'shan' , \"shan't\" , 'shouldn' , \"shouldn't\" , 'wasn' , \"wasn't\" , 'weren' , \"weren't\" , 'won' , \"won't\" , 'wouldn' , \"wouldn't\" ]) Conditional preprocessing on text. Usage: preprocess ( text = \"Transfer learning with BERT!\" , lower = True , stem = True ) 'transfer learn bert' Parameters: Name Type Description Default text str String to preprocess. required lower bool Lower the text. Defaults to True. True stem bool Stem the text. Defaults to False. False filters str Filters to apply on text. required stopwords List List of words to filter out. Defaults to STOPWORDS. ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] Returns: Type Description str Preprocessed string. Source code in mlops_with_ml/data.py def preprocess ( text : str , lower : bool = True , stem : bool = False , stopwords : List = config . STOPWORDS , ) -> str : \"\"\"Conditional preprocessing on text. Usage: ```python preprocess(text=\"Transfer learning with BERT!\", lower=True, stem=True) ``` <pre> 'transfer learn bert' </pre> Args: text (str): String to preprocess. lower (bool, optional): Lower the text. Defaults to True. stem (bool, optional): Stem the text. Defaults to False. filters (str, optional): Filters to apply on text. stopwords (List, optional): List of words to filter out. Defaults to STOPWORDS. Returns: Preprocessed string. \"\"\" # Lower if lower : text = text . lower () # Remove stopwords if len ( stopwords ): pattern = re . compile ( r \"\\b(\" + r \"|\" . join ( stopwords ) + r \")\\b\\s*\" ) text = pattern . sub ( \"\" , text ) # Spacing and filters text = re . sub ( r \"([! \\\" '#$%&()*\\+,-./:;<=>?@ \\\\ \\[\\]^_`{|}~])\" , r \" \\1 \" , text ) # add spacing between objects to be filtered text = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , text ) # remove non alphanumeric chars text = re . sub ( \" +\" , \" \" , text ) # remove multiple spaces text = text . strip () # Remove links text = re . sub ( r \"http\\S+\" , \"\" , text ) # Stemming if stem : stemmer = Stemmer () text = \" \" . join ([ stemmer . stem ( word ) for word in text . split ( \" \" )]) return text","title":"Data"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.CNNTextDataset","text":"Create torch.utils.data.Dataset objects to use for efficiently feeding data into our models. Usage: # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size ) Source code in mlops_with_ml/data.py class CNNTextDataset ( torch . utils . data . Dataset ): \"\"\"Create `torch.utils.data.Dataset` objects to use for efficiently feeding data into our models. Usage: ```python # Create dataset X, y = data dataset = CNNTextDataset(X=X, y=y, max_filter_size=max_filter_size) # Create dataloaders dataloader = dataset.create_dataloader(batch_size=batch_size) ``` \"\"\" def __init__ ( self , X , y , max_filter_size ): self . X = X self . y = y self . max_filter_size = max_filter_size def __len__ ( self ): return len ( self . y ) def __str__ ( self ): return f \"<Dataset(N= { len ( self ) } )>\" def __getitem__ ( self , index : int ) -> List : X = self . X [ index ] y = self . y [ index ] return [ X , y ] def collate_fn ( self , batch : List ) -> Tuple : \"\"\"Processing on a batch. It's used to override the default `collate_fn` in `torch.utils.data.DataLoader`. Args: batch (List): List of inputs and outputs. Returns: Processed inputs and outputs. \"\"\" # Get inputs batch = np . array ( batch , dtype = object ) X = batch [:, 0 ] y = np . stack ( batch [:, 1 ], axis = 0 ) # Pad inputs X = pad_sequences ( sequences = X , max_seq_len = self . max_filter_size ) # Cast X = torch . LongTensor ( X . astype ( np . int32 )) y = torch . FloatTensor ( y . astype ( np . int32 )) return X , y def create_dataloader ( self , batch_size : int , shuffle : bool = False , drop_last : bool = False ) -> torch . utils . data . DataLoader : \"\"\"Create dataloaders to load batches with. Usage: ```python # Create dataset X, y = data dataset = CNNTextDataset(X=X, y=y, max_filter_size=max_filter_size) # Create dataloaders dataloader = dataset.create_dataloader(batch_size=batch_size) ``` Args: batch_size (int): Number of samples per batch. shuffle (bool, optional): Shuffle each batch. Defaults to False. drop_last (bool, optional): Drop the last batch if it's less than `batch_size`. Defaults to False. Returns: Torch dataloader to load batches with. \"\"\" return torch . utils . data . DataLoader ( dataset = self , batch_size = batch_size , collate_fn = self . collate_fn , shuffle = shuffle , drop_last = drop_last , pin_memory = True , )","title":"CNNTextDataset"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.CNNTextDataset.collate_fn","text":"Processing on a batch. It's used to override the default collate_fn in torch.utils.data.DataLoader . Parameters: Name Type Description Default batch List List of inputs and outputs. required Returns: Type Description Tuple Processed inputs and outputs. Source code in mlops_with_ml/data.py def collate_fn ( self , batch : List ) -> Tuple : \"\"\"Processing on a batch. It's used to override the default `collate_fn` in `torch.utils.data.DataLoader`. Args: batch (List): List of inputs and outputs. Returns: Processed inputs and outputs. \"\"\" # Get inputs batch = np . array ( batch , dtype = object ) X = batch [:, 0 ] y = np . stack ( batch [:, 1 ], axis = 0 ) # Pad inputs X = pad_sequences ( sequences = X , max_seq_len = self . max_filter_size ) # Cast X = torch . LongTensor ( X . astype ( np . int32 )) y = torch . FloatTensor ( y . astype ( np . int32 )) return X , y","title":"collate_fn()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.CNNTextDataset.create_dataloader","text":"Create dataloaders to load batches with. Usage: # Create dataset X , y = data dataset = CNNTextDataset ( X = X , y = y , max_filter_size = max_filter_size ) # Create dataloaders dataloader = dataset . create_dataloader ( batch_size = batch_size ) Parameters: Name Type Description Default batch_size int Number of samples per batch. required shuffle bool Shuffle each batch. Defaults to False. False drop_last bool Drop the last batch if it's less than batch_size . Defaults to False. False Returns: Type Description DataLoader Torch dataloader to load batches with. Source code in mlops_with_ml/data.py def create_dataloader ( self , batch_size : int , shuffle : bool = False , drop_last : bool = False ) -> torch . utils . data . DataLoader : \"\"\"Create dataloaders to load batches with. Usage: ```python # Create dataset X, y = data dataset = CNNTextDataset(X=X, y=y, max_filter_size=max_filter_size) # Create dataloaders dataloader = dataset.create_dataloader(batch_size=batch_size) ``` Args: batch_size (int): Number of samples per batch. shuffle (bool, optional): Shuffle each batch. Defaults to False. drop_last (bool, optional): Drop the last batch if it's less than `batch_size`. Defaults to False. Returns: Torch dataloader to load batches with. \"\"\" return torch . utils . data . DataLoader ( dataset = self , batch_size = batch_size , collate_fn = self . collate_fn , shuffle = shuffle , drop_last = drop_last , pin_memory = True , )","title":"create_dataloader()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.LabelEncoder","text":"Encode labels into unique indices. Usage: # Encode labels label_encoder = LabelEncoder () label_encoder . fit ( labels ) y = label_encoder . encode ( labels ) Source code in mlops_with_ml/data.py class LabelEncoder : \"\"\"Encode labels into unique indices. Usage: ```python # Encode labels label_encoder = LabelEncoder() label_encoder.fit(labels) y = label_encoder.encode(labels) ``` \"\"\" def __init__ ( self , class_to_index : dict = {}): self . class_to_index = class_to_index or {} # mutable defaults ;) self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) def __len__ ( self ): return len ( self . class_to_index ) def save ( self , fp : str ): with open ( fp , \"w\" ) as fp : contents = { \"class_to_index\" : self . class_to_index } json . dump ( contents , fp , indent = 4 , sort_keys = False ) @classmethod def load ( cls , fp : str ): with open ( fp ) as fp : kwargs = json . load ( fp = fp ) return cls ( ** kwargs )","title":"LabelEncoder"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.MultiClassLabelEncoder","text":"Encode labels into unique indices for multi-class classification. Source code in mlops_with_ml/data.py class MultiClassLabelEncoder ( LabelEncoder ): \"\"\"Encode labels into unique indices for multi-class classification. \"\"\" def __str__ ( self ): return f \"<MultiClassLabelEncoder(num_classes= { len ( self ) } )>\" def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( y ) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of classes. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" encoded = np . zeros (( len ( y )), dtype = int ) for i , item in enumerate ( y ): encoded [ i ] = self . class_to_index [ item ] return encoded def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a collection of class indices. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): classes . append ( self . index_to_class [ item ]) return classes","title":"MultiClassLabelEncoder"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.MultiClassLabelEncoder.decode","text":"Decode a collection of class indices. Parameters: Name Type Description Default y np.ndarray Labels as (multilabel) one-hot encodings required Returns: Type Description List[List[str]] List of original labels for each output. Source code in mlops_with_ml/data.py def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a collection of class indices. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): classes . append ( self . index_to_class [ item ]) return classes","title":"decode()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.MultiClassLabelEncoder.encode","text":"Encode a collection of classes. Parameters: Name Type Description Default y pd.Series Collection of labels as a pandas Series object. required Returns: Type Description ndarray Labels as (multilabel) one-hot encodings Source code in mlops_with_ml/data.py def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of classes. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" encoded = np . zeros (( len ( y )), dtype = int ) for i , item in enumerate ( y ): encoded [ i ] = self . class_to_index [ item ] return encoded","title":"encode()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.MultiClassLabelEncoder.fit","text":"Learn label mappings from a series of class labels. Parameters: Name Type Description Default y Sequence Collection of labels as a pandas Series object. required Source code in mlops_with_ml/data.py def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( y ) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self","title":"fit()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.MultiLabelLabelEncoder","text":"Encode labels into unique indices for multi-label classification. Source code in mlops_with_ml/data.py class MultiLabelLabelEncoder ( LabelEncoder ): \"\"\"Encode labels into unique indices for multi-label classification. \"\"\" def __str__ ( self ): return f \"<MultiLabelLabelEncoder(num_classes= { len ( self ) } )>\" def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( list ( itertools . chain . from_iterable ( y ))) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of labels using (multilabel) one-hot encoding. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" y_one_hot = np . zeros (( len ( y ), len ( self . class_to_index )), dtype = int ) for i , item in enumerate ( y ): for class_ in item : y_one_hot [ i ][ self . class_to_index [ class_ ]] = 1 return y_one_hot def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a (multilabel) one-hot encoding into corresponding labels. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): indices = np . where ( np . asarray ( item ) == 1 )[ 0 ] classes . append ([ self . index_to_class [ index ] for index in indices ]) return classes","title":"MultiLabelLabelEncoder"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.MultiLabelLabelEncoder.decode","text":"Decode a (multilabel) one-hot encoding into corresponding labels. Parameters: Name Type Description Default y np.ndarray Labels as (multilabel) one-hot encodings required Returns: Type Description List[List[str]] List of original labels for each output. Source code in mlops_with_ml/data.py def decode ( self , y : np . ndarray ) -> List [ List [ str ]]: \"\"\"Decode a (multilabel) one-hot encoding into corresponding labels. Args: y (np.ndarray): Labels as (multilabel) one-hot encodings Returns: List of original labels for each output. \"\"\" classes = [] for i , item in enumerate ( y ): indices = np . where ( np . asarray ( item ) == 1 )[ 0 ] classes . append ([ self . index_to_class [ index ] for index in indices ]) return classes","title":"decode()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.MultiLabelLabelEncoder.encode","text":"Encode a collection of labels using (multilabel) one-hot encoding. Parameters: Name Type Description Default y pd.Series Collection of labels as a pandas Series object. required Returns: Type Description ndarray Labels as (multilabel) one-hot encodings Source code in mlops_with_ml/data.py def encode ( self , y : pd . Series ) -> np . ndarray : \"\"\"Encode a collection of labels using (multilabel) one-hot encoding. Args: y (pd.Series): Collection of labels as a pandas Series object. Returns: Labels as (multilabel) one-hot encodings \"\"\" y_one_hot = np . zeros (( len ( y ), len ( self . class_to_index )), dtype = int ) for i , item in enumerate ( y ): for class_ in item : y_one_hot [ i ][ self . class_to_index [ class_ ]] = 1 return y_one_hot","title":"encode()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.MultiLabelLabelEncoder.fit","text":"Learn label mappings from a series of class labels. Parameters: Name Type Description Default y Sequence Collection of labels as a pandas Series object. required Source code in mlops_with_ml/data.py def fit ( self , y : Sequence ): \"\"\"Learn label mappings from a series of class labels. Args: y (Sequence): Collection of labels as a pandas Series object. \"\"\" classes = np . unique ( list ( itertools . chain . from_iterable ( y ))) for i , class_ in enumerate ( classes ): self . class_to_index [ class_ ] = i self . index_to_class = { v : k for k , v in self . class_to_index . items ()} self . classes = list ( self . class_to_index . keys ()) return self","title":"fit()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.Stemmer","text":"Source code in mlops_with_ml/data.py class Stemmer ( PorterStemmer ): def stem ( self , word ): if ( self . mode == self . NLTK_EXTENSIONS and word in self . pool ): # pragma: no cover, nltk return self . pool [ word ] if ( self . mode != self . ORIGINAL_ALGORITHM and len ( word ) <= 2 ): # pragma: no cover, nltk # With this line, strings of length 1 or 2 don't go through # the stemming process, although no mention is made of this # in the published algorithm. return word stem = self . _step1a ( word ) stem = self . _step1b ( stem ) stem = self . _step1c ( stem ) stem = self . _step2 ( stem ) stem = self . _step3 ( stem ) stem = self . _step4 ( stem ) stem = self . _step5a ( stem ) stem = self . _step5b ( stem ) return stem","title":"Stemmer"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.Stemmer.stem","text":":param to_lowercase: if to_lowercase=True the word always lowercase Source code in mlops_with_ml/data.py def stem ( self , word ): if ( self . mode == self . NLTK_EXTENSIONS and word in self . pool ): # pragma: no cover, nltk return self . pool [ word ] if ( self . mode != self . ORIGINAL_ALGORITHM and len ( word ) <= 2 ): # pragma: no cover, nltk # With this line, strings of length 1 or 2 don't go through # the stemming process, although no mention is made of this # in the published algorithm. return word stem = self . _step1a ( word ) stem = self . _step1b ( stem ) stem = self . _step1c ( stem ) stem = self . _step2 ( stem ) stem = self . _step3 ( stem ) stem = self . _step4 ( stem ) stem = self . _step5a ( stem ) stem = self . _step5b ( stem ) return stem","title":"stem()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.Tokenizer","text":"Tokenize a feature using a built vocabulary. Usage: tokenizer = Tokenizer ( char_level = char_level ) tokenizer . fit_on_texts ( texts = X ) X = np . array ( tokenizer . texts_to_sequences ( X ), dtype = object ) Source code in mlops_with_ml/data.py class Tokenizer : \"\"\"Tokenize a feature using a built vocabulary. Usage: ```python tokenizer = Tokenizer(char_level=char_level) tokenizer.fit_on_texts(texts=X) X = np.array(tokenizer.texts_to_sequences(X), dtype=object) ``` \"\"\" def __init__ ( self , char_level : bool , num_tokens : int = None , pad_token : str = \"<PAD>\" , oov_token : str = \"<UNK>\" , token_to_index : dict = None , ): self . char_level = char_level self . separator = \"\" if self . char_level else \" \" if num_tokens : num_tokens -= 2 # pad + unk tokens self . num_tokens = num_tokens self . pad_token = pad_token self . oov_token = oov_token if not token_to_index : token_to_index = { pad_token : 0 , oov_token : 1 } self . token_to_index = token_to_index self . index_to_token = { v : k for k , v in self . token_to_index . items ()} def __len__ ( self ): return len ( self . token_to_index ) def __str__ ( self ): return f \"<Tokenizer(num_tokens= { len ( self ) } )>\" def fit_on_texts ( self , texts : List ): \"\"\"Learn token mappings from a list of texts. Args: texts (List): List of texts made of tokens. \"\"\" if not self . char_level : texts = [ text . split ( \" \" ) for text in texts ] all_tokens = [ token for text in texts for token in text ] counts = Counter ( all_tokens ) . most_common ( self . num_tokens ) self . min_token_freq = counts [ - 1 ][ 1 ] for token , count in counts : index = len ( self ) self . token_to_index [ token ] = index self . index_to_token [ index ] = token return self def texts_to_sequences ( self , texts : List ) -> List [ List ]: \"\"\"Convert a list of texts to a lists of arrays of indices. Args: texts (List): List of texts to tokenize and map to indices. Returns: A list of mapped sequences (list of indices). \"\"\" sequences = [] for text in texts : if not self . char_level : text = text . split ( \" \" ) sequence = [] for token in text : sequence . append ( self . token_to_index . get ( token , self . token_to_index [ self . oov_token ]) ) sequences . append ( sequence ) return sequences def sequences_to_texts ( self , sequences : List ) -> List : \"\"\"Convert a lists of arrays of indices to a list of texts. Args: sequences (List): list of mapped tokens to convert back to text. Returns: Mapped text from index tokens. \"\"\" texts = [] for sequence in sequences : text = [] for index in sequence : text . append ( self . index_to_token . get ( index , self . oov_token )) texts . append ( self . separator . join ([ token for token in text ])) return texts def save ( self , fp : str ): with open ( fp , \"w\" ) as fp : contents = { \"char_level\" : self . char_level , \"oov_token\" : self . oov_token , \"token_to_index\" : self . token_to_index , } json . dump ( contents , fp , indent = 4 , sort_keys = False ) @classmethod def load ( cls , fp : str ): with open ( fp ) as fp : kwargs = json . load ( fp = fp ) return cls ( ** kwargs )","title":"Tokenizer"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.Tokenizer.fit_on_texts","text":"Learn token mappings from a list of texts. Parameters: Name Type Description Default texts List List of texts made of tokens. required Source code in mlops_with_ml/data.py def fit_on_texts ( self , texts : List ): \"\"\"Learn token mappings from a list of texts. Args: texts (List): List of texts made of tokens. \"\"\" if not self . char_level : texts = [ text . split ( \" \" ) for text in texts ] all_tokens = [ token for text in texts for token in text ] counts = Counter ( all_tokens ) . most_common ( self . num_tokens ) self . min_token_freq = counts [ - 1 ][ 1 ] for token , count in counts : index = len ( self ) self . token_to_index [ token ] = index self . index_to_token [ index ] = token return self","title":"fit_on_texts()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.Tokenizer.sequences_to_texts","text":"Convert a lists of arrays of indices to a list of texts. Parameters: Name Type Description Default sequences List list of mapped tokens to convert back to text. required Returns: Type Description List Mapped text from index tokens. Source code in mlops_with_ml/data.py def sequences_to_texts ( self , sequences : List ) -> List : \"\"\"Convert a lists of arrays of indices to a list of texts. Args: sequences (List): list of mapped tokens to convert back to text. Returns: Mapped text from index tokens. \"\"\" texts = [] for sequence in sequences : text = [] for index in sequence : text . append ( self . index_to_token . get ( index , self . oov_token )) texts . append ( self . separator . join ([ token for token in text ])) return texts","title":"sequences_to_texts()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.Tokenizer.texts_to_sequences","text":"Convert a list of texts to a lists of arrays of indices. Parameters: Name Type Description Default texts List List of texts to tokenize and map to indices. required Returns: Type Description List[List] A list of mapped sequences (list of indices). Source code in mlops_with_ml/data.py def texts_to_sequences ( self , texts : List ) -> List [ List ]: \"\"\"Convert a list of texts to a lists of arrays of indices. Args: texts (List): List of texts to tokenize and map to indices. Returns: A list of mapped sequences (list of indices). \"\"\" sequences = [] for text in texts : if not self . char_level : text = text . split ( \" \" ) sequence = [] for token in text : sequence . append ( self . token_to_index . get ( token , self . token_to_index [ self . oov_token ]) ) sequences . append ( sequence ) return sequences","title":"texts_to_sequences()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.compute_features","text":"Compute features to use for training. Parameters: Name Type Description Default params Namespace Input parameters for operations. required Source code in mlops_with_ml/data.py def compute_features ( params : Namespace ) -> None : \"\"\"Compute features to use for training. Args: params (Namespace): Input parameters for operations. \"\"\" # Set up utils . set_seed ( seed = params . seed ) # Load data projects_url = \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/projects.json\" projects = utils . load_json_from_url ( url = projects_url ) df = pd . DataFrame ( projects ) # Compute features df [ \"text\" ] = df . title + \" \" + df . description df . drop ( columns = [ \"title\" , \"description\" ], inplace = True ) df = df [[ \"id\" , \"created_on\" , \"text\" , \"tags\" ]] # Save features = df . to_dict ( orient = \"records\" ) df_dict_fp = Path ( config . DATA_RAW_DIR , \"features.json\" ) utils . save_dict ( d = features , filepath = df_dict_fp ) return df , features","title":"compute_features()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.filter_items","text":"Filter a list using inclusion and exclusion lists of items. Parameters: Name Type Description Default items List List of items to apply filters. required include List List of items to include. Defaults to []. [] exclude List List of items to filter out. Defaults to []. [] Returns: Type Description List Filtered list of items. Usage: # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = list ( tags_dict . keys ()), exclude = config . EXCLDUE , ) Source code in mlops_with_ml/data.py def filter_items ( items : List , include : List = [], exclude : List = []) -> List : \"\"\"Filter a list using inclusion and exclusion lists of items. Args: items (List): List of items to apply filters. include (List, optional): List of items to include. Defaults to []. exclude (List, optional): List of items to filter out. Defaults to []. Returns: Filtered list of items. Usage: ```python # Filter tags for each project df.tags = df.tags.apply( filter_items, include=list(tags_dict.keys()), exclude=config.EXCLDUE, ) ``` \"\"\" # Filter filtered = [ item for item in items if item in include and item not in exclude ] return filtered","title":"filter_items()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.iterative_train_test_split","text":"Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Parameters: Name Type Description Default X pd.Series Input features as a pandas Series object. required y np.ndarray One-hot encoded labels. required train_size float Proportion of data for first split. Defaults to 0.7. 0.7 Returns: Type Description Tuple Two stratified splits based on specified proportions. Source code in mlops_with_ml/data.py def iterative_train_test_split ( X : pd . Series , y : np . ndarray , train_size : float = 0.7 ) -> Tuple : \"\"\"Custom iterative train test split which 'maintains balanced representation with respect to order-th label combinations.' Args: X (pd.Series): Input features as a pandas Series object. y (np.ndarray): One-hot encoded labels. train_size (float, optional): Proportion of data for first split. Defaults to 0.7. Returns: Two stratified splits based on specified proportions. \"\"\" stratifier = IterativeStratification ( n_splits = 2 , order = 1 , sample_distribution_per_fold = [ 1.0 - train_size , train_size , ], ) train_indices , test_indices = next ( stratifier . split ( X , y )) X_train , y_train = X [ train_indices ], y [ train_indices ] X_test , y_test = X [ test_indices ], y [ test_indices ] return X_train , X_test , y_train , y_test","title":"iterative_train_test_split()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.prepare","text":"Prepare the raw data. Parameters: Name Type Description Default df pd.DataFrame Pandas DataFrame with data. required include List list of tags to include. [] exclude List list of tags to exclude. [] min_tag_freq int Minimum frequency of tags required. Defaults to 30. 30 Returns: Type Description Tuple A cleaned dataframe and dictionary of tags and counts above the frequency threshold. Source code in mlops_with_ml/data.py def prepare ( df : pd . DataFrame , include : List = [], exclude : List = [], min_tag_freq : int = 30 ) -> Tuple : \"\"\"Prepare the raw data. Args: df (pd.DataFrame): Pandas DataFrame with data. include (List): list of tags to include. exclude (List): list of tags to exclude. min_tag_freq (int, optional): Minimum frequency of tags required. Defaults to 30. Returns: A cleaned dataframe and dictionary of tags and counts above the frequency threshold. \"\"\" # Filter tags for each project df . tags = df . tags . apply ( filter_items , include = include , exclude = exclude ) tags = Counter ( itertools . chain . from_iterable ( df . tags . values )) # Filter tags that have fewer than `min_tag_freq` occurrences tags_above_freq = Counter ( tag for tag in tags . elements () if tags [ tag ] >= min_tag_freq ) tags_below_freq = Counter ( tag for tag in tags . elements () if tags [ tag ] < min_tag_freq ) df . tags = df . tags . apply ( filter_items , include = list ( tags_above_freq . keys ())) # Remove projects with no more remaining relevant tags df = df [ df . tags . map ( len ) > 0 ] return df , tags_above_freq , tags_below_freq","title":"prepare()"},{"location":"mlops_with_ml/data/#mlops_with_ml.data.preprocess","text":"Conditional preprocessing on text. Usage: preprocess ( text = \"Transfer learning with BERT!\" , lower = True , stem = True ) 'transfer learn bert' Parameters: Name Type Description Default text str String to preprocess. required lower bool Lower the text. Defaults to True. True stem bool Stem the text. Defaults to False. False filters str Filters to apply on text. required stopwords List List of words to filter out. Defaults to STOPWORDS. ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] Returns: Type Description str Preprocessed string. Source code in mlops_with_ml/data.py def preprocess ( text : str , lower : bool = True , stem : bool = False , stopwords : List = config . STOPWORDS , ) -> str : \"\"\"Conditional preprocessing on text. Usage: ```python preprocess(text=\"Transfer learning with BERT!\", lower=True, stem=True) ``` <pre> 'transfer learn bert' </pre> Args: text (str): String to preprocess. lower (bool, optional): Lower the text. Defaults to True. stem (bool, optional): Stem the text. Defaults to False. filters (str, optional): Filters to apply on text. stopwords (List, optional): List of words to filter out. Defaults to STOPWORDS. Returns: Preprocessed string. \"\"\" # Lower if lower : text = text . lower () # Remove stopwords if len ( stopwords ): pattern = re . compile ( r \"\\b(\" + r \"|\" . join ( stopwords ) + r \")\\b\\s*\" ) text = pattern . sub ( \"\" , text ) # Spacing and filters text = re . sub ( r \"([! \\\" '#$%&()*\\+,-./:;<=>?@ \\\\ \\[\\]^_`{|}~])\" , r \" \\1 \" , text ) # add spacing between objects to be filtered text = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , text ) # remove non alphanumeric chars text = re . sub ( \" +\" , \" \" , text ) # remove multiple spaces text = text . strip () # Remove links text = re . sub ( r \"http\\S+\" , \"\" , text ) # Stemming if stem : stemmer = Stemmer () text = \" \" . join ([ stemmer . stem ( word ) for word in text . split ( \" \" )]) return text","title":"preprocess()"},{"location":"mlops_with_ml/eval/","text":"compare_tags ( texts , tags , artifacts , test_type ) Compare ground truth with predicted tags. Parameters: Name Type Description Default texts List List of input texts to predict on. required tags Dict List of ground truth tags for each input. required artifacts Dict Artifacts needed for inference. required test_type str Type of test (INV, DIR, MFT, etc.) required Returns: Type Description List Results with inputs, predictions and success status. Source code in mlops_with_ml/eval.py def compare_tags ( texts : str , tags : List , artifacts : Dict , test_type : str ) -> List : \"\"\"Compare ground truth with predicted tags. Args: texts (List): List of input texts to predict on. tags (Dict): List of ground truth tags for each input. artifacts (Dict): Artifacts needed for inference. test_type (str): Type of test (INV, DIR, MFT, etc.) Returns: List: Results with inputs, predictions and success status. \"\"\" # Predict predictions = predict . predict ( texts = texts , artifacts = artifacts ) # Evaluate results = { \"passed\" : [], \"failed\" : []} for i , prediction in enumerate ( predictions ): result = { \"input\" : { \"text\" : texts [ i ], \"tags\" : tags [ i ]}, \"prediction\" : predictions [ i ], \"type\" : test_type , } if all ( tag in prediction [ \"predicted_tags\" ] for tag in tags [ i ] ): # pragma: no cover, may not have any in test cases results [ \"passed\" ] . append ( result ) else : # pragma: no cover, may not have any in test cases results [ \"failed\" ] . append ( result ) return results evaluate ( df , artifacts , device = device ( type = 'cpu' )) Evaluate performance on data. Parameters: Name Type Description Default df pd.DataFrame Dataframe (used for slicing). required artifacts Dict Artifacts needed for inference. required device torch.device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Tuple Ground truth and predicted labels, performance. Source code in mlops_with_ml/eval.py def evaluate ( df : pd . DataFrame , artifacts : Dict , device : torch . device = torch . device ( \"cpu\" ), ) -> Tuple : \"\"\"Evaluate performance on data. Args: df (pd.DataFrame): Dataframe (used for slicing). artifacts (Dict): Artifacts needed for inference. device (torch.device): Device to run model on. Defaults to CPU. Returns: Ground truth and predicted labels, performance. \"\"\" # Artifacts params = artifacts [ \"params\" ] model = artifacts [ \"model\" ] tokenizer = artifacts [ \"tokenizer\" ] label_encoder = artifacts [ \"label_encoder\" ] model = model . to ( device ) classes = label_encoder . classes # Create dataloader X = np . array ( tokenizer . texts_to_sequences ( df . text . to_numpy ()), dtype = \"object\" ) y = label_encoder . encode ( df . tags ) dataset = data . CNNTextDataset ( X = X , y = y , max_filter_size = int ( params . max_filter_size )) dataloader = dataset . create_dataloader ( batch_size = int ( params . batch_size )) # Determine predictions using threshold trainer = train . Trainer ( model = model , device = device ) y_true , y_prob = trainer . predict_step ( dataloader = dataloader ) y_pred = np . array ( [ np . where ( prob >= float ( params . threshold ), 1 , 0 ) for prob in y_prob ] ) # Evaluate performance performance = {} performance = get_metrics ( df = df , y_true = y_true , y_pred = y_pred , classes = classes ) performance [ \"behavioral\" ] = get_behavioral_report ( artifacts = artifacts ) return y_true , y_pred , performance get_behavioral_report ( artifacts ) Assess failure rate by performing behavioral tests on our trained model. Parameters: Name Type Description Default artifacts Dict Artifacts needed for inference. required Returns: Type Description Dict Results of behavioral tests. Source code in mlops_with_ml/eval.py def get_behavioral_report ( artifacts : Dict ) -> Dict : \"\"\"Assess failure rate by performing behavioral tests on our trained model. Args: artifacts (Dict): Artifacts needed for inference. Returns: Dict: Results of behavioral tests. \"\"\" results = { \"passed\" : [], \"failed\" : []} # INVariance via verb injection (changes should not affect outputs) tokens = [ \"revolutionized\" , \"disrupted\" , \"accelerated\" ] tags = [[ \"transformers\" ], [ \"transformers\" ], [ \"transformers\" ]] texts = [ f \"Transformers have { token } the ML field.\" for token in tokens ] for status , items in compare_tags ( texts = texts , tags = tags , artifacts = artifacts , test_type = \"INV\" ) . items (): results [ status ] . extend ( items ) # INVariance via misspelling tokens = [ \"generative adverseril network\" , \"generated adversarial networks\" ] tags = [[ \"generative-adversarial-networks\" ], [ \"generative-adversarial-networks\" ]] texts = [ f \" { token } are very popular in machine learning projects.\" for token in tokens ] for status , items in compare_tags ( texts = texts , tags = tags , artifacts = artifacts , test_type = \"INV\" ) . items (): results [ status ] . extend ( items ) # DIRectional expectations (changes with known outputs) tokens = [ \"TensorFlow\" , \"Huggingface\" ] tags = [ [ \"tensorflow\" , \"transformers\" ], [ \"huggingface\" , \"transformers\" ], ] texts = [ f \"A { token } implementation of transformers.\" for token in tokens ] for status , items in compare_tags ( texts = texts , tags = tags , artifacts = artifacts , test_type = \"DIR\" ) . items (): results [ status ] . extend ( items ) # Minimum Functionality Tests (simple input/output pairs) tokens = [ \"transformers\" , \"graph neural networks\" ] tags = [[ \"transformers\" ], [ \"graph-neural-networks\" ]] texts = [ f \" { token } have revolutionized machine learning.\" for token in tokens ] for status , items in compare_tags ( texts = texts , tags = tags , artifacts = artifacts , test_type = \"MFT\" ) . items (): results [ status ] . extend ( items ) # Behavioral score score = len ( results [ \"passed\" ]) / float ( len ( results [ \"passed\" ]) + len ( results [ \"failed\" ]) ) return { \"score\" : score , \"results\" : results } get_metrics ( y_true , y_pred , classes , df = None ) Calculate metrics for fine-grained performance evaluation. Parameters: Name Type Description Default y_true np.ndarray True class labels. required y_pred np.ndarray Predicted class labels. required classes List List of all unique classes. required df pd.DataFrame dataframe used for slicing. None Returns: Type Description Dict Dictionary of fine-grained performance metrics. Source code in mlops_with_ml/eval.py def get_metrics ( y_true : np . ndarray , y_pred : np . ndarray , classes : List , df : pd . DataFrame = None ) -> Dict : \"\"\"Calculate metrics for fine-grained performance evaluation. Args: y_true (np.ndarray): True class labels. y_pred (np.ndarray): Predicted class labels. classes (List): List of all unique classes. df (pd.DataFrame, optional): dataframe used for slicing. Returns: Dictionary of fine-grained performance metrics. \"\"\" # Performance metrics = { \"overall\" : {}, \"class\" : {}} # Overall metrics overall_metrics = precision_recall_fscore_support ( y_true , y_pred , average = \"weighted\" ) metrics [ \"overall\" ][ \"precision\" ] = overall_metrics [ 0 ] metrics [ \"overall\" ][ \"recall\" ] = overall_metrics [ 1 ] metrics [ \"overall\" ][ \"f1\" ] = overall_metrics [ 2 ] metrics [ \"overall\" ][ \"num_samples\" ] = np . float64 ( len ( y_true )) # Per-class metrics class_metrics = precision_recall_fscore_support ( y_true , y_pred , average = None ) for i in range ( len ( classes )): metrics [ \"class\" ][ classes [ i ]] = { \"precision\" : class_metrics [ 0 ][ i ], \"recall\" : class_metrics [ 1 ][ i ], \"f1\" : class_metrics [ 2 ][ i ], \"num_samples\" : np . float64 ( class_metrics [ 3 ][ i ]), } # Slicing metrics if df is not None : # Slices slicing_functions = [ cv_transformers , short_text ] applier = PandasSFApplier ( slicing_functions ) slices = applier . apply ( df ) # Score slices # Use snorkel.analysis.Scorer for multiclass tasks # Naive implementation for our multilabel task # based on snorkel.analysis.Scorer metrics [ \"slices\" ] = {} metrics [ \"slices\" ][ \"class\" ] = {} for slice_name in slices . dtype . names : mask = slices [ slice_name ] . astype ( bool ) if sum ( mask ): # pragma: no cover, test set may not have enough samples for slicing slice_metrics = precision_recall_fscore_support ( y_true [ mask ], y_pred [ mask ], average = \"micro\" ) metrics [ \"slices\" ][ \"class\" ][ slice_name ] = {} metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"precision\" ] = slice_metrics [ 0 ] metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"recall\" ] = slice_metrics [ 1 ] metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"f1\" ] = slice_metrics [ 2 ] metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"num_samples\" ] = len ( y_true [ mask ] ) # Weighted overall slice metrics metrics [ \"slices\" ][ \"overall\" ] = {} for metric in [ \"precision\" , \"recall\" , \"f1\" ]: metrics [ \"slices\" ][ \"overall\" ][ metric ] = np . mean ( list ( itertools . chain . from_iterable ( [ [ metrics [ \"slices\" ][ \"class\" ][ slice_name ][ metric ]] * metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"num_samples\" ] for slice_name in metrics [ \"slices\" ][ \"class\" ] ] ) ) ) return metrics","title":"Eval"},{"location":"mlops_with_ml/eval/#mlops_with_ml.eval.compare_tags","text":"Compare ground truth with predicted tags. Parameters: Name Type Description Default texts List List of input texts to predict on. required tags Dict List of ground truth tags for each input. required artifacts Dict Artifacts needed for inference. required test_type str Type of test (INV, DIR, MFT, etc.) required Returns: Type Description List Results with inputs, predictions and success status. Source code in mlops_with_ml/eval.py def compare_tags ( texts : str , tags : List , artifacts : Dict , test_type : str ) -> List : \"\"\"Compare ground truth with predicted tags. Args: texts (List): List of input texts to predict on. tags (Dict): List of ground truth tags for each input. artifacts (Dict): Artifacts needed for inference. test_type (str): Type of test (INV, DIR, MFT, etc.) Returns: List: Results with inputs, predictions and success status. \"\"\" # Predict predictions = predict . predict ( texts = texts , artifacts = artifacts ) # Evaluate results = { \"passed\" : [], \"failed\" : []} for i , prediction in enumerate ( predictions ): result = { \"input\" : { \"text\" : texts [ i ], \"tags\" : tags [ i ]}, \"prediction\" : predictions [ i ], \"type\" : test_type , } if all ( tag in prediction [ \"predicted_tags\" ] for tag in tags [ i ] ): # pragma: no cover, may not have any in test cases results [ \"passed\" ] . append ( result ) else : # pragma: no cover, may not have any in test cases results [ \"failed\" ] . append ( result ) return results","title":"compare_tags()"},{"location":"mlops_with_ml/eval/#mlops_with_ml.eval.evaluate","text":"Evaluate performance on data. Parameters: Name Type Description Default df pd.DataFrame Dataframe (used for slicing). required artifacts Dict Artifacts needed for inference. required device torch.device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Tuple Ground truth and predicted labels, performance. Source code in mlops_with_ml/eval.py def evaluate ( df : pd . DataFrame , artifacts : Dict , device : torch . device = torch . device ( \"cpu\" ), ) -> Tuple : \"\"\"Evaluate performance on data. Args: df (pd.DataFrame): Dataframe (used for slicing). artifacts (Dict): Artifacts needed for inference. device (torch.device): Device to run model on. Defaults to CPU. Returns: Ground truth and predicted labels, performance. \"\"\" # Artifacts params = artifacts [ \"params\" ] model = artifacts [ \"model\" ] tokenizer = artifacts [ \"tokenizer\" ] label_encoder = artifacts [ \"label_encoder\" ] model = model . to ( device ) classes = label_encoder . classes # Create dataloader X = np . array ( tokenizer . texts_to_sequences ( df . text . to_numpy ()), dtype = \"object\" ) y = label_encoder . encode ( df . tags ) dataset = data . CNNTextDataset ( X = X , y = y , max_filter_size = int ( params . max_filter_size )) dataloader = dataset . create_dataloader ( batch_size = int ( params . batch_size )) # Determine predictions using threshold trainer = train . Trainer ( model = model , device = device ) y_true , y_prob = trainer . predict_step ( dataloader = dataloader ) y_pred = np . array ( [ np . where ( prob >= float ( params . threshold ), 1 , 0 ) for prob in y_prob ] ) # Evaluate performance performance = {} performance = get_metrics ( df = df , y_true = y_true , y_pred = y_pred , classes = classes ) performance [ \"behavioral\" ] = get_behavioral_report ( artifacts = artifacts ) return y_true , y_pred , performance","title":"evaluate()"},{"location":"mlops_with_ml/eval/#mlops_with_ml.eval.get_behavioral_report","text":"Assess failure rate by performing behavioral tests on our trained model. Parameters: Name Type Description Default artifacts Dict Artifacts needed for inference. required Returns: Type Description Dict Results of behavioral tests. Source code in mlops_with_ml/eval.py def get_behavioral_report ( artifacts : Dict ) -> Dict : \"\"\"Assess failure rate by performing behavioral tests on our trained model. Args: artifacts (Dict): Artifacts needed for inference. Returns: Dict: Results of behavioral tests. \"\"\" results = { \"passed\" : [], \"failed\" : []} # INVariance via verb injection (changes should not affect outputs) tokens = [ \"revolutionized\" , \"disrupted\" , \"accelerated\" ] tags = [[ \"transformers\" ], [ \"transformers\" ], [ \"transformers\" ]] texts = [ f \"Transformers have { token } the ML field.\" for token in tokens ] for status , items in compare_tags ( texts = texts , tags = tags , artifacts = artifacts , test_type = \"INV\" ) . items (): results [ status ] . extend ( items ) # INVariance via misspelling tokens = [ \"generative adverseril network\" , \"generated adversarial networks\" ] tags = [[ \"generative-adversarial-networks\" ], [ \"generative-adversarial-networks\" ]] texts = [ f \" { token } are very popular in machine learning projects.\" for token in tokens ] for status , items in compare_tags ( texts = texts , tags = tags , artifacts = artifacts , test_type = \"INV\" ) . items (): results [ status ] . extend ( items ) # DIRectional expectations (changes with known outputs) tokens = [ \"TensorFlow\" , \"Huggingface\" ] tags = [ [ \"tensorflow\" , \"transformers\" ], [ \"huggingface\" , \"transformers\" ], ] texts = [ f \"A { token } implementation of transformers.\" for token in tokens ] for status , items in compare_tags ( texts = texts , tags = tags , artifacts = artifacts , test_type = \"DIR\" ) . items (): results [ status ] . extend ( items ) # Minimum Functionality Tests (simple input/output pairs) tokens = [ \"transformers\" , \"graph neural networks\" ] tags = [[ \"transformers\" ], [ \"graph-neural-networks\" ]] texts = [ f \" { token } have revolutionized machine learning.\" for token in tokens ] for status , items in compare_tags ( texts = texts , tags = tags , artifacts = artifacts , test_type = \"MFT\" ) . items (): results [ status ] . extend ( items ) # Behavioral score score = len ( results [ \"passed\" ]) / float ( len ( results [ \"passed\" ]) + len ( results [ \"failed\" ]) ) return { \"score\" : score , \"results\" : results }","title":"get_behavioral_report()"},{"location":"mlops_with_ml/eval/#mlops_with_ml.eval.get_metrics","text":"Calculate metrics for fine-grained performance evaluation. Parameters: Name Type Description Default y_true np.ndarray True class labels. required y_pred np.ndarray Predicted class labels. required classes List List of all unique classes. required df pd.DataFrame dataframe used for slicing. None Returns: Type Description Dict Dictionary of fine-grained performance metrics. Source code in mlops_with_ml/eval.py def get_metrics ( y_true : np . ndarray , y_pred : np . ndarray , classes : List , df : pd . DataFrame = None ) -> Dict : \"\"\"Calculate metrics for fine-grained performance evaluation. Args: y_true (np.ndarray): True class labels. y_pred (np.ndarray): Predicted class labels. classes (List): List of all unique classes. df (pd.DataFrame, optional): dataframe used for slicing. Returns: Dictionary of fine-grained performance metrics. \"\"\" # Performance metrics = { \"overall\" : {}, \"class\" : {}} # Overall metrics overall_metrics = precision_recall_fscore_support ( y_true , y_pred , average = \"weighted\" ) metrics [ \"overall\" ][ \"precision\" ] = overall_metrics [ 0 ] metrics [ \"overall\" ][ \"recall\" ] = overall_metrics [ 1 ] metrics [ \"overall\" ][ \"f1\" ] = overall_metrics [ 2 ] metrics [ \"overall\" ][ \"num_samples\" ] = np . float64 ( len ( y_true )) # Per-class metrics class_metrics = precision_recall_fscore_support ( y_true , y_pred , average = None ) for i in range ( len ( classes )): metrics [ \"class\" ][ classes [ i ]] = { \"precision\" : class_metrics [ 0 ][ i ], \"recall\" : class_metrics [ 1 ][ i ], \"f1\" : class_metrics [ 2 ][ i ], \"num_samples\" : np . float64 ( class_metrics [ 3 ][ i ]), } # Slicing metrics if df is not None : # Slices slicing_functions = [ cv_transformers , short_text ] applier = PandasSFApplier ( slicing_functions ) slices = applier . apply ( df ) # Score slices # Use snorkel.analysis.Scorer for multiclass tasks # Naive implementation for our multilabel task # based on snorkel.analysis.Scorer metrics [ \"slices\" ] = {} metrics [ \"slices\" ][ \"class\" ] = {} for slice_name in slices . dtype . names : mask = slices [ slice_name ] . astype ( bool ) if sum ( mask ): # pragma: no cover, test set may not have enough samples for slicing slice_metrics = precision_recall_fscore_support ( y_true [ mask ], y_pred [ mask ], average = \"micro\" ) metrics [ \"slices\" ][ \"class\" ][ slice_name ] = {} metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"precision\" ] = slice_metrics [ 0 ] metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"recall\" ] = slice_metrics [ 1 ] metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"f1\" ] = slice_metrics [ 2 ] metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"num_samples\" ] = len ( y_true [ mask ] ) # Weighted overall slice metrics metrics [ \"slices\" ][ \"overall\" ] = {} for metric in [ \"precision\" , \"recall\" , \"f1\" ]: metrics [ \"slices\" ][ \"overall\" ][ metric ] = np . mean ( list ( itertools . chain . from_iterable ( [ [ metrics [ \"slices\" ][ \"class\" ][ slice_name ][ metric ]] * metrics [ \"slices\" ][ \"class\" ][ slice_name ][ \"num_samples\" ] for slice_name in metrics [ \"slices\" ][ \"class\" ] ] ) ) ) return metrics","title":"get_metrics()"},{"location":"mlops_with_ml/main/","text":"behavioral_reevaluation ( model_dir = PosixPath ( '/home/namnd00/projects/mlops-with-ml/models' )) Reevaluate existing runs on current behavioral tests in eval.py. This is possible since behavioral tests are inputs applied to black box models and compared with expected outputs. There is not dependency on data or model versions. Parameters: Name Type Description Default model_dir Path location of model artifacts. PosixPath('/home/namnd00/projects/mlops-with-ml/models') Exceptions: Type Description ValueError Run id doesn't exist in experiment. Source code in mlops_with_ml/main.py @app . command () def behavioral_reevaluation ( model_dir : Path = config . MODEL_DIR , ): # pragma: no cover, requires changing existing runs \"\"\"Reevaluate existing runs on current behavioral tests in eval.py. This is possible since behavioral tests are inputs applied to black box models and compared with expected outputs. There is not dependency on data or model versions. Args: model_dir (Path): location of model artifacts. Raises: ValueError: Run id doesn't exist in experiment. \"\"\" # Generate behavioral report artifacts = load_artifacts ( model_dir = model_dir ) artifacts [ \"performance\" ][ \"behavioral\" ] = eval . get_behavioral_report ( artifacts = artifacts ) mlflow . log_metric ( \"behavioral_score\" , artifacts [ \"performance\" ][ \"behavioral\" ][ \"score\" ] ) # Log updated performance utils . save_dict ( artifacts [ \"performance\" ], Path ( model_dir , \"performance.json\" )) compute_features ( params_fp = PosixPath ( '/home/namnd00/projects/mlops-with-ml/configs/params.json' )) Compute and save features for training. Parameters: Name Type Description Default params_fp Path Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to config/params.json . PosixPath('/home/namnd00/projects/mlops-with-ml/configs/params.json') Source code in mlops_with_ml/main.py @app . command () def compute_features ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), ) -> None : \"\"\"Compute and save features for training. Args: params_fp (Path, optional): Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to `config/params.json`. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Compute features data . compute_features ( params = params ) logger . info ( \"\u2705 Computed features!\" ) delete_experiment ( experiment_name ) Delete an experiment with name experiment_name . Parameters: Name Type Description Default experiment_name str Name of the experiment. required Source code in mlops_with_ml/main.py def delete_experiment ( experiment_name : str ): \"\"\"Delete an experiment with name `experiment_name`. Args: experiment_name (str): Name of the experiment. \"\"\" client = mlflow . tracking . MlflowClient () experiment_id = client . get_experiment_by_name ( experiment_name ) . experiment_id client . delete_experiment ( experiment_id = experiment_id ) logger . info ( f \"\u2705 Deleted experiment { experiment_name } !\" ) diff ( author = 'NamND' , repo = 'mlops' , tag_a = 'workspace' , tag_b = '' ) Difference between two release TAGs. Source code in mlops_with_ml/main.py @app . command () def diff ( author : str = config . AUTHOR , repo : str = config . REPO , tag_a : str = \"workspace\" , tag_b : str = \"\" , ): # pragma: no cover, can't be certain what diffs will exist \"\"\"Difference between two release TAGs.\"\"\" # Tag b if tag_b == \"\" : tags_url = f \"https://api.github.com/repos/ { author } / { repo } /tags\" tag_b = utils . load_json_from_url ( url = tags_url )[ 0 ][ \"name\" ] logger . info ( f \"Comparing { tag_a } with { tag_b } :\" ) # Params params_a = params ( author = author , repo = repo , tag = tag_a , verbose = False ) params_b = params ( author = author , repo = repo , tag = tag_b , verbose = False ) params_diff = utils . dict_diff ( d_a = params_a , d_b = params_b , d_a_name = tag_a , d_b_name = tag_b ) logger . info ( f \"Parameter differences: { json . dumps ( params_diff , indent = 2 ) } \" ) # Performance performance_a = performance ( author = author , repo = repo , tag = tag_a , verbose = False ) performance_b = performance ( author = author , repo = repo , tag = tag_b , verbose = False ) performance_diff = utils . dict_diff ( d_a = performance_a , d_b = performance_b , d_a_name = tag_a , d_b_name = tag_b ) logger . info ( f \"Performance differences: { json . dumps ( performance_diff , indent = 2 ) } \" ) return params_diff , performance_diff download_auxiliary_data () Load auxiliary data from URL and save to local drive. Source code in mlops_with_ml/main.py @app . command () def download_auxiliary_data (): \"\"\"Load auxiliary data from URL and save to local drive.\"\"\" # Download auxiliary data tags_url = \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/tags.json\" tags = utils . load_json_from_url ( url = tags_url ) # Save data tags_fp = Path ( config . DATA_RAW_DIR , \"tags.json\" ) utils . save_dict ( d = tags , filepath = tags_fp ) logger . info ( \"\u2705 Auxiliary data downloaded!\" ) get_historical_features () Retrieve historical features for training. Source code in mlops_with_ml/main.py @app . command () def get_historical_features (): \"\"\"Retrieve historical features for training.\"\"\" # Entities to pull data for (should dynamically read this from somewhere) project_ids = [ 1 , 2 , 3 ] now = datetime . now () timestamps = [ datetime ( now . year , now . month , now . day )] * len ( project_ids ) entity_df = pd . DataFrame . from_dict ( { \"id\" : project_ids , \"event_timestamp\" : timestamps } ) # Get historical features store = FeatureStore ( repo_path = Path ( config . BASE_DIR , \"features\" )) training_df = store . get_historical_features ( entity_df = entity_df , feature_refs = [ \"project_details:text\" , \"project_details:tags\" ], ) . to_df () # Store in location for training task to pick up print ( training_df . head ()) load_artifacts ( run_id , device = device ( type = 'cpu' )) Load artifacts for current model. Parameters: Name Type Description Default run_id str ID of the model run to load artifacts. required device torch.device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Dict Artifacts needed for inference. Source code in mlops_with_ml/main.py def load_artifacts ( run_id : str , device : torch . device = torch . device ( \"cpu\" )) -> Dict : \"\"\"Load artifacts for current model. Args: run_id (str): ID of the model run to load artifacts. device (torch.device): Device to run model on. Defaults to CPU. Returns: Artifacts needed for inference. \"\"\" # Load artifacts artifact_uri = mlflow . get_run ( run_id = run_id ) . info . artifact_uri . split ( \"file://\" )[ - 1 ] params = Namespace ( ** utils . load_dict ( filepath = Path ( artifact_uri , \"params.json\" ))) label_encoder = data . MultiLabelLabelEncoder . load ( fp = Path ( artifact_uri , \"label_encoder.json\" ) ) tokenizer = data . Tokenizer . load ( fp = Path ( artifact_uri , \"tokenizer.json\" )) model_state = torch . load ( Path ( artifact_uri , \"model.pt\" ), map_location = device ) performance = utils . load_dict ( filepath = Path ( artifact_uri , \"performance.json\" )) # Initialize model model = models . initialize_model ( params = params , vocab_size = len ( tokenizer ), num_classes = len ( label_encoder ) ) model . load_state_dict ( model_state ) return { \"params\" : params , \"label_encoder\" : label_encoder , \"tokenizer\" : tokenizer , \"model\" : model , \"performance\" : performance , } optimize ( params_fp = PosixPath ( '/home/namnd00/projects/mlops-with-ml/configs/params.json' ), study_name = 'optimization' , num_trials = 100 ) Optimize a subset of hyperparameters towards an objective. This saves the best trial's parameters into config/params.json . Parameters: Name Type Description Default params_fp Path Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to config/params.json . PosixPath('/home/namnd00/projects/mlops-with-ml/configs/params.json') study_name str Name of the study to save trial runs under. Defaults to optimization . 'optimization' num_trials int Number of trials to run. Defaults to 100. 100 Source code in mlops_with_ml/main.py @app . command () def optimize ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), study_name : Optional [ str ] = \"optimization\" , num_trials : int = 100 , ) -> None : \"\"\"Optimize a subset of hyperparameters towards an objective. This saves the best trial's parameters into `config/params.json`. Args: params_fp (Path, optional): Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to `config/params.json`. study_name (str, optional): Name of the study to save trial runs under. Defaults to `optimization`. num_trials (int, optional): Number of trials to run. Defaults to 100. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Optimize pruner = optuna . pruners . MedianPruner ( n_startup_trials = 5 , n_warmup_steps = 5 ) study = optuna . create_study ( study_name = study_name , direction = \"maximize\" , pruner = pruner ) mlflow_callback = MLflowCallback ( tracking_uri = mlflow . get_tracking_uri (), metric_name = \"f1\" ) study . optimize ( lambda trial : train . objective ( params , trial ), n_trials = num_trials , callbacks = [ mlflow_callback ], ) # All trials trials_df = study . trials_dataframe () trials_df = trials_df . sort_values ([ \"value\" ], ascending = False ) # Best trial logger . info ( f \"Best value (f1): { study . best_trial . value } \" ) params = { ** params . __dict__ , ** study . best_trial . params , \"threshold\" : study . best_trial . user_attrs [ \"threshold\" ], } utils . save_dict ( params , params_fp , cls = NumpyEncoder ) logger . info ( json . dumps ( params , indent = 2 , cls = NumpyEncoder )) params ( run_id ) Configured parametes for a specific run ID. Source code in mlops_with_ml/main.py @app . command () def params ( run_id : str ) -> Dict : \"\"\"Configured parametes for a specific run ID.\"\"\" params = load_artifacts ( run_id = run_id )[ \"params\" ] logger . info ( json . dumps ( params , indent = 2 )) return params performance ( run_id ) Performance summary for a specific run ID. Source code in mlops_with_ml/main.py @app . command () def performance ( run_id : str ) -> Dict : \"\"\"Performance summary for a specific run ID.\"\"\" performance = load_artifacts ( run_id = run_id )[ \"performance\" ] logger . info ( json . dumps ( performance , indent = 2 )) return performance predict_tags ( text , run_id ) Warning Make sure that you have a trained model first! Parameters: Name Type Description Default text str Input text to predict tags for. required run_id str ID of the model run to load artifacts. required Exceptions: Type Description ValueError Run id doesn't exist in experiment. Returns: Type Description Dict Predicted tags for input text. Source code in mlops_with_ml/main.py @app . command () def predict_tags ( text : str , run_id : str ) -> Dict : \"\"\"Predict tags for a give input text using a trained model. Warning: Make sure that you have a trained model first! Args: text (str): Input text to predict tags for. run_id (str): ID of the model run to load artifacts. Raises: ValueError: Run id doesn't exist in experiment. Returns: Predicted tags for input text. \"\"\" # Predict artifacts = load_artifacts ( run_id = run_id ) prediction = predict . predict ( texts = [ text ], artifacts = artifacts ) logger . info ( json . dumps ( prediction , indent = 2 )) return prediction train_model ( params_fp = PosixPath ( '/home/namnd00/projects/mlops-with-ml/configs/params.json' ), experiment_name = 'best' , run_name = 'model' ) Train a model using the specified parameters. Parameters: Name Type Description Default params_fp Path Parameters to use for training. Defaults to config/params.json . PosixPath('/home/namnd00/projects/mlops-with-ml/configs/params.json') experiment_name str Name of the experiment to save the run to. Defaults to best . 'best' run_name str Name of the run. Defaults to model . 'model' Source code in mlops_with_ml/main.py @app . command () def train_model ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), experiment_name : Optional [ str ] = \"best\" , run_name : Optional [ str ] = \"model\" , ) -> None : \"\"\"Train a model using the specified parameters. Args: params_fp (Path, optional): Parameters to use for training. Defaults to `config/params.json`. experiment_name (str, optional): Name of the experiment to save the run to. Defaults to `best`. run_name (str, optional): Name of the run. Defaults to `model`. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Start run mlflow . set_experiment ( experiment_name = experiment_name ) with mlflow . start_run ( run_name = run_name ): run_id = mlflow . active_run () . info . run_id logger . info ( f \"Run ID: { run_id } \" ) # Train artifacts = train . train ( params = params ) # Set tags tags = {} mlflow . set_tags ( tags ) # Log metrics performance = artifacts [ \"performance\" ] logger . info ( json . dumps ( performance [ \"overall\" ], indent = 2 )) metrics = { \"precision\" : performance [ \"overall\" ][ \"precision\" ], \"recall\" : performance [ \"overall\" ][ \"recall\" ], \"f1\" : performance [ \"overall\" ][ \"f1\" ], \"best_val_loss\" : artifacts [ \"loss\" ], \"behavioral_score\" : performance [ \"behavioral\" ][ \"score\" ], \"slices_f1\" : performance [ \"slices\" ][ \"overall\" ][ \"f1\" ], } mlflow . log_metrics ( metrics ) # Log artifacts with tempfile . TemporaryDirectory () as dp : utils . save_dict ( vars ( artifacts [ \"params\" ]), Path ( dp , \"params.json\" ), cls = NumpyEncoder ) utils . save_dict ( performance , Path ( dp , \"performance.json\" )) artifacts [ \"label_encoder\" ] . save ( Path ( dp , \"label_encoder.json\" )) artifacts [ \"tokenizer\" ] . save ( Path ( dp , \"tokenizer.json\" )) torch . save ( artifacts [ \"model\" ] . state_dict (), Path ( dp , \"model.pt\" )) mlflow . log_artifacts ( dp ) mlflow . log_params ( vars ( artifacts [ \"params\" ]))","title":"Operations"},{"location":"mlops_with_ml/main/#mlops_with_ml.main.behavioral_reevaluation","text":"Reevaluate existing runs on current behavioral tests in eval.py. This is possible since behavioral tests are inputs applied to black box models and compared with expected outputs. There is not dependency on data or model versions. Parameters: Name Type Description Default model_dir Path location of model artifacts. PosixPath('/home/namnd00/projects/mlops-with-ml/models') Exceptions: Type Description ValueError Run id doesn't exist in experiment. Source code in mlops_with_ml/main.py @app . command () def behavioral_reevaluation ( model_dir : Path = config . MODEL_DIR , ): # pragma: no cover, requires changing existing runs \"\"\"Reevaluate existing runs on current behavioral tests in eval.py. This is possible since behavioral tests are inputs applied to black box models and compared with expected outputs. There is not dependency on data or model versions. Args: model_dir (Path): location of model artifacts. Raises: ValueError: Run id doesn't exist in experiment. \"\"\" # Generate behavioral report artifacts = load_artifacts ( model_dir = model_dir ) artifacts [ \"performance\" ][ \"behavioral\" ] = eval . get_behavioral_report ( artifacts = artifacts ) mlflow . log_metric ( \"behavioral_score\" , artifacts [ \"performance\" ][ \"behavioral\" ][ \"score\" ] ) # Log updated performance utils . save_dict ( artifacts [ \"performance\" ], Path ( model_dir , \"performance.json\" ))","title":"behavioral_reevaluation()"},{"location":"mlops_with_ml/main/#mlops_with_ml.main.compute_features","text":"Compute and save features for training. Parameters: Name Type Description Default params_fp Path Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to config/params.json . PosixPath('/home/namnd00/projects/mlops-with-ml/configs/params.json') Source code in mlops_with_ml/main.py @app . command () def compute_features ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), ) -> None : \"\"\"Compute and save features for training. Args: params_fp (Path, optional): Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to `config/params.json`. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Compute features data . compute_features ( params = params ) logger . info ( \"\u2705 Computed features!\" )","title":"compute_features()"},{"location":"mlops_with_ml/main/#mlops_with_ml.main.delete_experiment","text":"Delete an experiment with name experiment_name . Parameters: Name Type Description Default experiment_name str Name of the experiment. required Source code in mlops_with_ml/main.py def delete_experiment ( experiment_name : str ): \"\"\"Delete an experiment with name `experiment_name`. Args: experiment_name (str): Name of the experiment. \"\"\" client = mlflow . tracking . MlflowClient () experiment_id = client . get_experiment_by_name ( experiment_name ) . experiment_id client . delete_experiment ( experiment_id = experiment_id ) logger . info ( f \"\u2705 Deleted experiment { experiment_name } !\" )","title":"delete_experiment()"},{"location":"mlops_with_ml/main/#mlops_with_ml.main.diff","text":"Difference between two release TAGs. Source code in mlops_with_ml/main.py @app . command () def diff ( author : str = config . AUTHOR , repo : str = config . REPO , tag_a : str = \"workspace\" , tag_b : str = \"\" , ): # pragma: no cover, can't be certain what diffs will exist \"\"\"Difference between two release TAGs.\"\"\" # Tag b if tag_b == \"\" : tags_url = f \"https://api.github.com/repos/ { author } / { repo } /tags\" tag_b = utils . load_json_from_url ( url = tags_url )[ 0 ][ \"name\" ] logger . info ( f \"Comparing { tag_a } with { tag_b } :\" ) # Params params_a = params ( author = author , repo = repo , tag = tag_a , verbose = False ) params_b = params ( author = author , repo = repo , tag = tag_b , verbose = False ) params_diff = utils . dict_diff ( d_a = params_a , d_b = params_b , d_a_name = tag_a , d_b_name = tag_b ) logger . info ( f \"Parameter differences: { json . dumps ( params_diff , indent = 2 ) } \" ) # Performance performance_a = performance ( author = author , repo = repo , tag = tag_a , verbose = False ) performance_b = performance ( author = author , repo = repo , tag = tag_b , verbose = False ) performance_diff = utils . dict_diff ( d_a = performance_a , d_b = performance_b , d_a_name = tag_a , d_b_name = tag_b ) logger . info ( f \"Performance differences: { json . dumps ( performance_diff , indent = 2 ) } \" ) return params_diff , performance_diff","title":"diff()"},{"location":"mlops_with_ml/main/#mlops_with_ml.main.download_auxiliary_data","text":"Load auxiliary data from URL and save to local drive. Source code in mlops_with_ml/main.py @app . command () def download_auxiliary_data (): \"\"\"Load auxiliary data from URL and save to local drive.\"\"\" # Download auxiliary data tags_url = \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/tags.json\" tags = utils . load_json_from_url ( url = tags_url ) # Save data tags_fp = Path ( config . DATA_RAW_DIR , \"tags.json\" ) utils . save_dict ( d = tags , filepath = tags_fp ) logger . info ( \"\u2705 Auxiliary data downloaded!\" )","title":"download_auxiliary_data()"},{"location":"mlops_with_ml/main/#mlops_with_ml.main.get_historical_features","text":"Retrieve historical features for training. Source code in mlops_with_ml/main.py @app . command () def get_historical_features (): \"\"\"Retrieve historical features for training.\"\"\" # Entities to pull data for (should dynamically read this from somewhere) project_ids = [ 1 , 2 , 3 ] now = datetime . now () timestamps = [ datetime ( now . year , now . month , now . day )] * len ( project_ids ) entity_df = pd . DataFrame . from_dict ( { \"id\" : project_ids , \"event_timestamp\" : timestamps } ) # Get historical features store = FeatureStore ( repo_path = Path ( config . BASE_DIR , \"features\" )) training_df = store . get_historical_features ( entity_df = entity_df , feature_refs = [ \"project_details:text\" , \"project_details:tags\" ], ) . to_df () # Store in location for training task to pick up print ( training_df . head ())","title":"get_historical_features()"},{"location":"mlops_with_ml/main/#mlops_with_ml.main.load_artifacts","text":"Load artifacts for current model. Parameters: Name Type Description Default run_id str ID of the model run to load artifacts. required device torch.device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Dict Artifacts needed for inference. Source code in mlops_with_ml/main.py def load_artifacts ( run_id : str , device : torch . device = torch . device ( \"cpu\" )) -> Dict : \"\"\"Load artifacts for current model. Args: run_id (str): ID of the model run to load artifacts. device (torch.device): Device to run model on. Defaults to CPU. Returns: Artifacts needed for inference. \"\"\" # Load artifacts artifact_uri = mlflow . get_run ( run_id = run_id ) . info . artifact_uri . split ( \"file://\" )[ - 1 ] params = Namespace ( ** utils . load_dict ( filepath = Path ( artifact_uri , \"params.json\" ))) label_encoder = data . MultiLabelLabelEncoder . load ( fp = Path ( artifact_uri , \"label_encoder.json\" ) ) tokenizer = data . Tokenizer . load ( fp = Path ( artifact_uri , \"tokenizer.json\" )) model_state = torch . load ( Path ( artifact_uri , \"model.pt\" ), map_location = device ) performance = utils . load_dict ( filepath = Path ( artifact_uri , \"performance.json\" )) # Initialize model model = models . initialize_model ( params = params , vocab_size = len ( tokenizer ), num_classes = len ( label_encoder ) ) model . load_state_dict ( model_state ) return { \"params\" : params , \"label_encoder\" : label_encoder , \"tokenizer\" : tokenizer , \"model\" : model , \"performance\" : performance , }","title":"load_artifacts()"},{"location":"mlops_with_ml/main/#mlops_with_ml.main.optimize","text":"Optimize a subset of hyperparameters towards an objective. This saves the best trial's parameters into config/params.json . Parameters: Name Type Description Default params_fp Path Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to config/params.json . PosixPath('/home/namnd00/projects/mlops-with-ml/configs/params.json') study_name str Name of the study to save trial runs under. Defaults to optimization . 'optimization' num_trials int Number of trials to run. Defaults to 100. 100 Source code in mlops_with_ml/main.py @app . command () def optimize ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), study_name : Optional [ str ] = \"optimization\" , num_trials : int = 100 , ) -> None : \"\"\"Optimize a subset of hyperparameters towards an objective. This saves the best trial's parameters into `config/params.json`. Args: params_fp (Path, optional): Location of parameters (just using num_samples, num_epochs, etc.) to use for training. Defaults to `config/params.json`. study_name (str, optional): Name of the study to save trial runs under. Defaults to `optimization`. num_trials (int, optional): Number of trials to run. Defaults to 100. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Optimize pruner = optuna . pruners . MedianPruner ( n_startup_trials = 5 , n_warmup_steps = 5 ) study = optuna . create_study ( study_name = study_name , direction = \"maximize\" , pruner = pruner ) mlflow_callback = MLflowCallback ( tracking_uri = mlflow . get_tracking_uri (), metric_name = \"f1\" ) study . optimize ( lambda trial : train . objective ( params , trial ), n_trials = num_trials , callbacks = [ mlflow_callback ], ) # All trials trials_df = study . trials_dataframe () trials_df = trials_df . sort_values ([ \"value\" ], ascending = False ) # Best trial logger . info ( f \"Best value (f1): { study . best_trial . value } \" ) params = { ** params . __dict__ , ** study . best_trial . params , \"threshold\" : study . best_trial . user_attrs [ \"threshold\" ], } utils . save_dict ( params , params_fp , cls = NumpyEncoder ) logger . info ( json . dumps ( params , indent = 2 , cls = NumpyEncoder ))","title":"optimize()"},{"location":"mlops_with_ml/main/#mlops_with_ml.main.params","text":"Configured parametes for a specific run ID. Source code in mlops_with_ml/main.py @app . command () def params ( run_id : str ) -> Dict : \"\"\"Configured parametes for a specific run ID.\"\"\" params = load_artifacts ( run_id = run_id )[ \"params\" ] logger . info ( json . dumps ( params , indent = 2 )) return params","title":"params()"},{"location":"mlops_with_ml/main/#mlops_with_ml.main.performance","text":"Performance summary for a specific run ID. Source code in mlops_with_ml/main.py @app . command () def performance ( run_id : str ) -> Dict : \"\"\"Performance summary for a specific run ID.\"\"\" performance = load_artifacts ( run_id = run_id )[ \"performance\" ] logger . info ( json . dumps ( performance , indent = 2 )) return performance","title":"performance()"},{"location":"mlops_with_ml/main/#mlops_with_ml.main.predict_tags","text":"Warning Make sure that you have a trained model first! Parameters: Name Type Description Default text str Input text to predict tags for. required run_id str ID of the model run to load artifacts. required Exceptions: Type Description ValueError Run id doesn't exist in experiment. Returns: Type Description Dict Predicted tags for input text. Source code in mlops_with_ml/main.py @app . command () def predict_tags ( text : str , run_id : str ) -> Dict : \"\"\"Predict tags for a give input text using a trained model. Warning: Make sure that you have a trained model first! Args: text (str): Input text to predict tags for. run_id (str): ID of the model run to load artifacts. Raises: ValueError: Run id doesn't exist in experiment. Returns: Predicted tags for input text. \"\"\" # Predict artifacts = load_artifacts ( run_id = run_id ) prediction = predict . predict ( texts = [ text ], artifacts = artifacts ) logger . info ( json . dumps ( prediction , indent = 2 )) return prediction","title":"predict_tags()"},{"location":"mlops_with_ml/main/#mlops_with_ml.main.train_model","text":"Train a model using the specified parameters. Parameters: Name Type Description Default params_fp Path Parameters to use for training. Defaults to config/params.json . PosixPath('/home/namnd00/projects/mlops-with-ml/configs/params.json') experiment_name str Name of the experiment to save the run to. Defaults to best . 'best' run_name str Name of the run. Defaults to model . 'model' Source code in mlops_with_ml/main.py @app . command () def train_model ( params_fp : Path = Path ( config . CONFIG_DIR , \"params.json\" ), experiment_name : Optional [ str ] = \"best\" , run_name : Optional [ str ] = \"model\" , ) -> None : \"\"\"Train a model using the specified parameters. Args: params_fp (Path, optional): Parameters to use for training. Defaults to `config/params.json`. experiment_name (str, optional): Name of the experiment to save the run to. Defaults to `best`. run_name (str, optional): Name of the run. Defaults to `model`. \"\"\" # Parameters params = Namespace ( ** utils . load_dict ( filepath = params_fp )) # Start run mlflow . set_experiment ( experiment_name = experiment_name ) with mlflow . start_run ( run_name = run_name ): run_id = mlflow . active_run () . info . run_id logger . info ( f \"Run ID: { run_id } \" ) # Train artifacts = train . train ( params = params ) # Set tags tags = {} mlflow . set_tags ( tags ) # Log metrics performance = artifacts [ \"performance\" ] logger . info ( json . dumps ( performance [ \"overall\" ], indent = 2 )) metrics = { \"precision\" : performance [ \"overall\" ][ \"precision\" ], \"recall\" : performance [ \"overall\" ][ \"recall\" ], \"f1\" : performance [ \"overall\" ][ \"f1\" ], \"best_val_loss\" : artifacts [ \"loss\" ], \"behavioral_score\" : performance [ \"behavioral\" ][ \"score\" ], \"slices_f1\" : performance [ \"slices\" ][ \"overall\" ][ \"f1\" ], } mlflow . log_metrics ( metrics ) # Log artifacts with tempfile . TemporaryDirectory () as dp : utils . save_dict ( vars ( artifacts [ \"params\" ]), Path ( dp , \"params.json\" ), cls = NumpyEncoder ) utils . save_dict ( performance , Path ( dp , \"performance.json\" )) artifacts [ \"label_encoder\" ] . save ( Path ( dp , \"label_encoder.json\" )) artifacts [ \"tokenizer\" ] . save ( Path ( dp , \"tokenizer.json\" )) torch . save ( artifacts [ \"model\" ] . state_dict (), Path ( dp , \"model.pt\" )) mlflow . log_artifacts ( dp ) mlflow . log_params ( vars ( artifacts [ \"params\" ]))","title":"train_model()"},{"location":"mlops_with_ml/models/","text":"CNN ( Module ) Source code in mlops_with_ml/models.py class CNN ( nn . Module ): def __init__ ( self , embedding_dim : int , vocab_size : int , num_filters : int , filter_sizes : list , hidden_dim : int , dropout_p : float , num_classes : int , padding_idx : int = 0 , ) -> None : \"\"\"A [convolutional neural network](https://madewithml.com/courses/foundations/convolutional-neural-networks/){:target=\"_blank\"} architecture created for natural language processing tasks where filters convolve across the given text inputs. ![text CNN](https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/foundations/embeddings/model.png) Usage: ```python # Initialize model filter_sizes = list(range(1, int(params.max_filter_size) + 1)) model = models.CNN( embedding_dim=int(params.embedding_dim), vocab_size=int(vocab_size), num_filters=int(params.num_filters), filter_sizes=filter_sizes, hidden_dim=int(params.hidden_dim), dropout_p=float(params.dropout_p), num_classes=int(num_classes), ) model = model.to(device) ``` Args: embedding_dim (int): Embedding dimension for tokens. vocab_size (int): Number of unique tokens in vocabulary. num_filters (int): Number of filters per filter size. filter_sizes (list): List of filter sizes for the CNN. hidden_dim (int): Hidden dimension for fully-connected (FC) layers. dropout_p (float): Dropout proportion for FC layers. num_classes (int): Number of unique classes to classify into. padding_idx (int, optional): Index representing the `<PAD>` token. Defaults to 0. \"\"\" super () . __init__ () # Initialize embeddings self . embeddings = nn . Embedding ( embedding_dim = embedding_dim , num_embeddings = vocab_size , padding_idx = padding_idx , ) # Conv weights self . filter_sizes = filter_sizes self . conv = nn . ModuleList ( [ nn . Conv1d ( in_channels = embedding_dim , out_channels = num_filters , kernel_size = f , ) for f in filter_sizes ] ) # FC weights self . dropout = nn . Dropout ( dropout_p ) self . fc1 = nn . Linear ( num_filters * len ( filter_sizes ), hidden_dim ) self . fc2 = nn . Linear ( hidden_dim , num_classes ) def forward ( self , inputs : List , channel_first : bool = False ) -> torch . Tensor : \"\"\"Forward pass. Args: inputs (List): List of inputs (by feature). channel_first (bool, optional): Channel dimension is first in inputs. Defaults to False. Returns: Outputs from the model. \"\"\" # Embed ( x_in ,) = inputs x_in = self . embeddings ( x_in ) if not channel_first : x_in = x_in . transpose ( 1 , 2 ) # (N, channels, sequence length) z = [] max_seq_len = x_in . shape [ 2 ] for i , f in enumerate ( self . filter_sizes ): # `SAME` padding padding_left = int ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ] ) / 2 ) padding_right = int ( math . ceil ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ] ) / 2 ) ) # Conv _z = self . conv [ i ]( F . pad ( x_in , ( padding_left , padding_right ))) # Pool _z = F . max_pool1d ( _z , _z . size ( 2 )) . squeeze ( 2 ) z . append ( _z ) # Concat outputs z = torch . cat ( z , 1 ) # FC z = self . fc1 ( z ) z = self . dropout ( z ) z = self . fc2 ( z ) return z __init__ ( self , embedding_dim , vocab_size , num_filters , filter_sizes , hidden_dim , dropout_p , num_classes , padding_idx = 0 ) special A convolutional neural network {:target=\"_blank\"} architecture created for natural language processing tasks where filters convolve across the given text inputs. Usage: # Initialize model filter_sizes = list ( range ( 1 , int ( params . max_filter_size ) + 1 )) model = models . CNN ( embedding_dim = int ( params . embedding_dim ), vocab_size = int ( vocab_size ), num_filters = int ( params . num_filters ), filter_sizes = filter_sizes , hidden_dim = int ( params . hidden_dim ), dropout_p = float ( params . dropout_p ), num_classes = int ( num_classes ), ) model = model . to ( device ) Parameters: Name Type Description Default embedding_dim int Embedding dimension for tokens. required vocab_size int Number of unique tokens in vocabulary. required num_filters int Number of filters per filter size. required filter_sizes list List of filter sizes for the CNN. required hidden_dim int Hidden dimension for fully-connected (FC) layers. required dropout_p float Dropout proportion for FC layers. required num_classes int Number of unique classes to classify into. required padding_idx int Index representing the <PAD> token. Defaults to 0. 0 Source code in mlops_with_ml/models.py def __init__ ( self , embedding_dim : int , vocab_size : int , num_filters : int , filter_sizes : list , hidden_dim : int , dropout_p : float , num_classes : int , padding_idx : int = 0 , ) -> None : \"\"\"A [convolutional neural network](https://madewithml.com/courses/foundations/convolutional-neural-networks/){:target=\"_blank\"} architecture created for natural language processing tasks where filters convolve across the given text inputs. ![text CNN](https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/foundations/embeddings/model.png) Usage: ```python # Initialize model filter_sizes = list(range(1, int(params.max_filter_size) + 1)) model = models.CNN( embedding_dim=int(params.embedding_dim), vocab_size=int(vocab_size), num_filters=int(params.num_filters), filter_sizes=filter_sizes, hidden_dim=int(params.hidden_dim), dropout_p=float(params.dropout_p), num_classes=int(num_classes), ) model = model.to(device) ``` Args: embedding_dim (int): Embedding dimension for tokens. vocab_size (int): Number of unique tokens in vocabulary. num_filters (int): Number of filters per filter size. filter_sizes (list): List of filter sizes for the CNN. hidden_dim (int): Hidden dimension for fully-connected (FC) layers. dropout_p (float): Dropout proportion for FC layers. num_classes (int): Number of unique classes to classify into. padding_idx (int, optional): Index representing the `<PAD>` token. Defaults to 0. \"\"\" super () . __init__ () # Initialize embeddings self . embeddings = nn . Embedding ( embedding_dim = embedding_dim , num_embeddings = vocab_size , padding_idx = padding_idx , ) # Conv weights self . filter_sizes = filter_sizes self . conv = nn . ModuleList ( [ nn . Conv1d ( in_channels = embedding_dim , out_channels = num_filters , kernel_size = f , ) for f in filter_sizes ] ) # FC weights self . dropout = nn . Dropout ( dropout_p ) self . fc1 = nn . Linear ( num_filters * len ( filter_sizes ), hidden_dim ) self . fc2 = nn . Linear ( hidden_dim , num_classes ) forward ( self , inputs , channel_first = False ) Forward pass. Parameters: Name Type Description Default inputs List List of inputs (by feature). required channel_first bool Channel dimension is first in inputs. Defaults to False. False Returns: Type Description Tensor Outputs from the model. Source code in mlops_with_ml/models.py def forward ( self , inputs : List , channel_first : bool = False ) -> torch . Tensor : \"\"\"Forward pass. Args: inputs (List): List of inputs (by feature). channel_first (bool, optional): Channel dimension is first in inputs. Defaults to False. Returns: Outputs from the model. \"\"\" # Embed ( x_in ,) = inputs x_in = self . embeddings ( x_in ) if not channel_first : x_in = x_in . transpose ( 1 , 2 ) # (N, channels, sequence length) z = [] max_seq_len = x_in . shape [ 2 ] for i , f in enumerate ( self . filter_sizes ): # `SAME` padding padding_left = int ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ] ) / 2 ) padding_right = int ( math . ceil ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ] ) / 2 ) ) # Conv _z = self . conv [ i ]( F . pad ( x_in , ( padding_left , padding_right ))) # Pool _z = F . max_pool1d ( _z , _z . size ( 2 )) . squeeze ( 2 ) z . append ( _z ) # Concat outputs z = torch . cat ( z , 1 ) # FC z = self . fc1 ( z ) z = self . dropout ( z ) z = self . fc2 ( z ) return z initialize_model ( params , vocab_size , num_classes , device = device ( type = 'cpu' )) Initialize a model using parameters (converted to appropriate data types). Parameters: Name Type Description Default params Namespace Parameters for data processing and training. required vocab_size int Size of the vocabulary. required num_classes int Number on unique classes. required device torch.device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Module Initialize torch model instance. Source code in mlops_with_ml/models.py def initialize_model ( params : Namespace , vocab_size : int , num_classes : int , device : torch . device = torch . device ( \"cpu\" ), ) -> nn . Module : \"\"\"Initialize a model using parameters (converted to appropriate data types). Args: params (Namespace): Parameters for data processing and training. vocab_size (int): Size of the vocabulary. num_classes (int): Number on unique classes. device (torch.device): Device to run model on. Defaults to CPU. Returns: Initialize torch model instance. \"\"\" # Initialize model filter_sizes = list ( range ( 1 , int ( params . max_filter_size ) + 1 )) model = CNN ( embedding_dim = int ( params . embedding_dim ), vocab_size = int ( vocab_size ), num_filters = int ( params . num_filters ), filter_sizes = filter_sizes , hidden_dim = int ( params . hidden_dim ), dropout_p = float ( params . dropout_p ), num_classes = int ( num_classes ), ) model = model . to ( device ) return model","title":"Models"},{"location":"mlops_with_ml/models/#mlops_with_ml.models.CNN","text":"Source code in mlops_with_ml/models.py class CNN ( nn . Module ): def __init__ ( self , embedding_dim : int , vocab_size : int , num_filters : int , filter_sizes : list , hidden_dim : int , dropout_p : float , num_classes : int , padding_idx : int = 0 , ) -> None : \"\"\"A [convolutional neural network](https://madewithml.com/courses/foundations/convolutional-neural-networks/){:target=\"_blank\"} architecture created for natural language processing tasks where filters convolve across the given text inputs. ![text CNN](https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/foundations/embeddings/model.png) Usage: ```python # Initialize model filter_sizes = list(range(1, int(params.max_filter_size) + 1)) model = models.CNN( embedding_dim=int(params.embedding_dim), vocab_size=int(vocab_size), num_filters=int(params.num_filters), filter_sizes=filter_sizes, hidden_dim=int(params.hidden_dim), dropout_p=float(params.dropout_p), num_classes=int(num_classes), ) model = model.to(device) ``` Args: embedding_dim (int): Embedding dimension for tokens. vocab_size (int): Number of unique tokens in vocabulary. num_filters (int): Number of filters per filter size. filter_sizes (list): List of filter sizes for the CNN. hidden_dim (int): Hidden dimension for fully-connected (FC) layers. dropout_p (float): Dropout proportion for FC layers. num_classes (int): Number of unique classes to classify into. padding_idx (int, optional): Index representing the `<PAD>` token. Defaults to 0. \"\"\" super () . __init__ () # Initialize embeddings self . embeddings = nn . Embedding ( embedding_dim = embedding_dim , num_embeddings = vocab_size , padding_idx = padding_idx , ) # Conv weights self . filter_sizes = filter_sizes self . conv = nn . ModuleList ( [ nn . Conv1d ( in_channels = embedding_dim , out_channels = num_filters , kernel_size = f , ) for f in filter_sizes ] ) # FC weights self . dropout = nn . Dropout ( dropout_p ) self . fc1 = nn . Linear ( num_filters * len ( filter_sizes ), hidden_dim ) self . fc2 = nn . Linear ( hidden_dim , num_classes ) def forward ( self , inputs : List , channel_first : bool = False ) -> torch . Tensor : \"\"\"Forward pass. Args: inputs (List): List of inputs (by feature). channel_first (bool, optional): Channel dimension is first in inputs. Defaults to False. Returns: Outputs from the model. \"\"\" # Embed ( x_in ,) = inputs x_in = self . embeddings ( x_in ) if not channel_first : x_in = x_in . transpose ( 1 , 2 ) # (N, channels, sequence length) z = [] max_seq_len = x_in . shape [ 2 ] for i , f in enumerate ( self . filter_sizes ): # `SAME` padding padding_left = int ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ] ) / 2 ) padding_right = int ( math . ceil ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ] ) / 2 ) ) # Conv _z = self . conv [ i ]( F . pad ( x_in , ( padding_left , padding_right ))) # Pool _z = F . max_pool1d ( _z , _z . size ( 2 )) . squeeze ( 2 ) z . append ( _z ) # Concat outputs z = torch . cat ( z , 1 ) # FC z = self . fc1 ( z ) z = self . dropout ( z ) z = self . fc2 ( z ) return z","title":"CNN"},{"location":"mlops_with_ml/models/#mlops_with_ml.models.CNN.__init__","text":"A convolutional neural network {:target=\"_blank\"} architecture created for natural language processing tasks where filters convolve across the given text inputs. Usage: # Initialize model filter_sizes = list ( range ( 1 , int ( params . max_filter_size ) + 1 )) model = models . CNN ( embedding_dim = int ( params . embedding_dim ), vocab_size = int ( vocab_size ), num_filters = int ( params . num_filters ), filter_sizes = filter_sizes , hidden_dim = int ( params . hidden_dim ), dropout_p = float ( params . dropout_p ), num_classes = int ( num_classes ), ) model = model . to ( device ) Parameters: Name Type Description Default embedding_dim int Embedding dimension for tokens. required vocab_size int Number of unique tokens in vocabulary. required num_filters int Number of filters per filter size. required filter_sizes list List of filter sizes for the CNN. required hidden_dim int Hidden dimension for fully-connected (FC) layers. required dropout_p float Dropout proportion for FC layers. required num_classes int Number of unique classes to classify into. required padding_idx int Index representing the <PAD> token. Defaults to 0. 0 Source code in mlops_with_ml/models.py def __init__ ( self , embedding_dim : int , vocab_size : int , num_filters : int , filter_sizes : list , hidden_dim : int , dropout_p : float , num_classes : int , padding_idx : int = 0 , ) -> None : \"\"\"A [convolutional neural network](https://madewithml.com/courses/foundations/convolutional-neural-networks/){:target=\"_blank\"} architecture created for natural language processing tasks where filters convolve across the given text inputs. ![text CNN](https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/images/foundations/embeddings/model.png) Usage: ```python # Initialize model filter_sizes = list(range(1, int(params.max_filter_size) + 1)) model = models.CNN( embedding_dim=int(params.embedding_dim), vocab_size=int(vocab_size), num_filters=int(params.num_filters), filter_sizes=filter_sizes, hidden_dim=int(params.hidden_dim), dropout_p=float(params.dropout_p), num_classes=int(num_classes), ) model = model.to(device) ``` Args: embedding_dim (int): Embedding dimension for tokens. vocab_size (int): Number of unique tokens in vocabulary. num_filters (int): Number of filters per filter size. filter_sizes (list): List of filter sizes for the CNN. hidden_dim (int): Hidden dimension for fully-connected (FC) layers. dropout_p (float): Dropout proportion for FC layers. num_classes (int): Number of unique classes to classify into. padding_idx (int, optional): Index representing the `<PAD>` token. Defaults to 0. \"\"\" super () . __init__ () # Initialize embeddings self . embeddings = nn . Embedding ( embedding_dim = embedding_dim , num_embeddings = vocab_size , padding_idx = padding_idx , ) # Conv weights self . filter_sizes = filter_sizes self . conv = nn . ModuleList ( [ nn . Conv1d ( in_channels = embedding_dim , out_channels = num_filters , kernel_size = f , ) for f in filter_sizes ] ) # FC weights self . dropout = nn . Dropout ( dropout_p ) self . fc1 = nn . Linear ( num_filters * len ( filter_sizes ), hidden_dim ) self . fc2 = nn . Linear ( hidden_dim , num_classes )","title":"__init__()"},{"location":"mlops_with_ml/models/#mlops_with_ml.models.CNN.forward","text":"Forward pass. Parameters: Name Type Description Default inputs List List of inputs (by feature). required channel_first bool Channel dimension is first in inputs. Defaults to False. False Returns: Type Description Tensor Outputs from the model. Source code in mlops_with_ml/models.py def forward ( self , inputs : List , channel_first : bool = False ) -> torch . Tensor : \"\"\"Forward pass. Args: inputs (List): List of inputs (by feature). channel_first (bool, optional): Channel dimension is first in inputs. Defaults to False. Returns: Outputs from the model. \"\"\" # Embed ( x_in ,) = inputs x_in = self . embeddings ( x_in ) if not channel_first : x_in = x_in . transpose ( 1 , 2 ) # (N, channels, sequence length) z = [] max_seq_len = x_in . shape [ 2 ] for i , f in enumerate ( self . filter_sizes ): # `SAME` padding padding_left = int ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ] ) / 2 ) padding_right = int ( math . ceil ( ( self . conv [ i ] . stride [ 0 ] * ( max_seq_len - 1 ) - max_seq_len + self . filter_sizes [ i ] ) / 2 ) ) # Conv _z = self . conv [ i ]( F . pad ( x_in , ( padding_left , padding_right ))) # Pool _z = F . max_pool1d ( _z , _z . size ( 2 )) . squeeze ( 2 ) z . append ( _z ) # Concat outputs z = torch . cat ( z , 1 ) # FC z = self . fc1 ( z ) z = self . dropout ( z ) z = self . fc2 ( z ) return z","title":"forward()"},{"location":"mlops_with_ml/models/#mlops_with_ml.models.initialize_model","text":"Initialize a model using parameters (converted to appropriate data types). Parameters: Name Type Description Default params Namespace Parameters for data processing and training. required vocab_size int Size of the vocabulary. required num_classes int Number on unique classes. required device torch.device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Module Initialize torch model instance. Source code in mlops_with_ml/models.py def initialize_model ( params : Namespace , vocab_size : int , num_classes : int , device : torch . device = torch . device ( \"cpu\" ), ) -> nn . Module : \"\"\"Initialize a model using parameters (converted to appropriate data types). Args: params (Namespace): Parameters for data processing and training. vocab_size (int): Size of the vocabulary. num_classes (int): Number on unique classes. device (torch.device): Device to run model on. Defaults to CPU. Returns: Initialize torch model instance. \"\"\" # Initialize model filter_sizes = list ( range ( 1 , int ( params . max_filter_size ) + 1 )) model = CNN ( embedding_dim = int ( params . embedding_dim ), vocab_size = int ( vocab_size ), num_filters = int ( params . num_filters ), filter_sizes = filter_sizes , hidden_dim = int ( params . hidden_dim ), dropout_p = float ( params . dropout_p ), num_classes = int ( num_classes ), ) model = model . to ( device ) return model","title":"initialize_model()"},{"location":"mlops_with_ml/predict/","text":"predict ( texts , artifacts , device = device ( type = 'cpu' )) Predict tags for an input text using the best model from the best experiment. Usage: texts = [ \"Transfer learning with BERT.\" ] artifacts = load_artifacts ( run_id = \"264ac530b78c42608e5dea1086bc2c73\" ) predict ( texts = texts , artifacts = artifacts ) [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] Note The input parameter texts can hold multiple input texts and so the resulting prediction dictionary will have len(texts) items. Parameters: Name Type Description Default texts List List of input texts to predict tags for. required artifacts Dict Artifacts needed for inference. required device torch.device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Dict Predicted tags for each of the input texts. Source code in mlops_with_ml/predict.py def predict ( texts : List , artifacts : Dict , device : torch . device = torch . device ( \"cpu\" ) ) -> Dict : \"\"\"Predict tags for an input text using the best model from the `best` experiment. Usage: ```python texts = [\"Transfer learning with BERT.\"] artifacts = load_artifacts(run_id=\"264ac530b78c42608e5dea1086bc2c73\") predict(texts=texts, artifacts=artifacts) ``` <pre> [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] </pre> Note: The input parameter `texts` can hold multiple input texts and so the resulting prediction dictionary will have `len(texts)` items. Args: texts (List): List of input texts to predict tags for. artifacts (Dict): Artifacts needed for inference. device (torch.device): Device to run model on. Defaults to CPU. Returns: Predicted tags for each of the input texts. \"\"\" # Retrieve artifacts params = artifacts [ \"params\" ] label_encoder = artifacts [ \"label_encoder\" ] tokenizer = artifacts [ \"tokenizer\" ] model = artifacts [ \"model\" ] # Prepare data preprocessed_texts = [ data . preprocess ( text , lower = bool ( strtobool ( str ( params . lower ))), # params.lower could be str/bool stem = bool ( strtobool ( str ( params . stem ))), ) for text in texts ] X = np . array ( tokenizer . texts_to_sequences ( preprocessed_texts ), dtype = \"object\" ) y_filler = np . zeros (( len ( X ), len ( label_encoder ))) dataset = data . CNNTextDataset ( X = X , y = y_filler , max_filter_size = int ( params . max_filter_size ) ) dataloader = dataset . create_dataloader ( batch_size = int ( params . batch_size )) # Get predictions trainer = train . Trainer ( model = model , device = device ) _ , y_prob = trainer . predict_step ( dataloader ) y_pred = [ np . where ( prob >= float ( params . threshold ), 1 , 0 ) for prob in y_prob ] tags = label_encoder . decode ( y_pred ) predictions = [ { \"input_text\" : texts [ i ], \"preprocessed_text\" : preprocessed_texts [ i ], \"predicted_tags\" : tags [ i ], } for i in range ( len ( tags )) ] return predictions","title":"Inference"},{"location":"mlops_with_ml/predict/#mlops_with_ml.predict.predict","text":"Predict tags for an input text using the best model from the best experiment. Usage: texts = [ \"Transfer learning with BERT.\" ] artifacts = load_artifacts ( run_id = \"264ac530b78c42608e5dea1086bc2c73\" ) predict ( texts = texts , artifacts = artifacts ) [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] Note The input parameter texts can hold multiple input texts and so the resulting prediction dictionary will have len(texts) items. Parameters: Name Type Description Default texts List List of input texts to predict tags for. required artifacts Dict Artifacts needed for inference. required device torch.device Device to run model on. Defaults to CPU. device(type='cpu') Returns: Type Description Dict Predicted tags for each of the input texts. Source code in mlops_with_ml/predict.py def predict ( texts : List , artifacts : Dict , device : torch . device = torch . device ( \"cpu\" ) ) -> Dict : \"\"\"Predict tags for an input text using the best model from the `best` experiment. Usage: ```python texts = [\"Transfer learning with BERT.\"] artifacts = load_artifacts(run_id=\"264ac530b78c42608e5dea1086bc2c73\") predict(texts=texts, artifacts=artifacts) ``` <pre> [ { \"input_text\": \"Transfer learning with BERT.\", \"preprocessed_text\": \"transfer learning bert\", \"predicted_tags\": [ \"attention\", \"language-modeling\", \"natural-language-processing\", \"transfer-learning\", \"transformers\" ] } ] </pre> Note: The input parameter `texts` can hold multiple input texts and so the resulting prediction dictionary will have `len(texts)` items. Args: texts (List): List of input texts to predict tags for. artifacts (Dict): Artifacts needed for inference. device (torch.device): Device to run model on. Defaults to CPU. Returns: Predicted tags for each of the input texts. \"\"\" # Retrieve artifacts params = artifacts [ \"params\" ] label_encoder = artifacts [ \"label_encoder\" ] tokenizer = artifacts [ \"tokenizer\" ] model = artifacts [ \"model\" ] # Prepare data preprocessed_texts = [ data . preprocess ( text , lower = bool ( strtobool ( str ( params . lower ))), # params.lower could be str/bool stem = bool ( strtobool ( str ( params . stem ))), ) for text in texts ] X = np . array ( tokenizer . texts_to_sequences ( preprocessed_texts ), dtype = \"object\" ) y_filler = np . zeros (( len ( X ), len ( label_encoder ))) dataset = data . CNNTextDataset ( X = X , y = y_filler , max_filter_size = int ( params . max_filter_size ) ) dataloader = dataset . create_dataloader ( batch_size = int ( params . batch_size )) # Get predictions trainer = train . Trainer ( model = model , device = device ) _ , y_prob = trainer . predict_step ( dataloader ) y_pred = [ np . where ( prob >= float ( params . threshold ), 1 , 0 ) for prob in y_prob ] tags = label_encoder . decode ( y_pred ) predictions = [ { \"input_text\" : texts [ i ], \"preprocessed_text\" : preprocessed_texts [ i ], \"predicted_tags\" : tags [ i ], } for i in range ( len ( tags )) ] return predictions","title":"predict()"},{"location":"mlops_with_ml/train/","text":"Trainer Object used to facilitate training. Source code in mlops_with_ml/train.py class Trainer : \"\"\"Object used to facilitate training.\"\"\" def __init__ ( self , model : nn . Module , device : torch . device = torch . device ( \"cpu\" ), loss_fn = None , optimizer = None , scheduler = None , trial : optuna . trial . _trial . Trial = None , ): # Set params self . model = model self . device = device self . loss_fn = loss_fn self . optimizer = optimizer self . scheduler = scheduler self . trial = trial def train_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Train step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to train mode self . model . train () loss = 0.0 # Iterate over train batches for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , targets = batch [: - 1 ], batch [ - 1 ] self . optimizer . zero_grad () # Reset gradients z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , targets ) # Define loss J . backward () # Backward pass self . optimizer . step () # Update weights # Cumulative Metrics loss += ( J . detach () . item () - loss ) / ( i + 1 ) return loss def eval_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Evaluation (val / test) step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () loss = 0.0 y_trues , y_probs = [], [] # Iterate over val batches with torch . inference_mode (): for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , y_true ) . item () # Cumulative Metrics loss += ( J - loss ) / ( i + 1 ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return loss , np . vstack ( y_trues ), np . vstack ( y_probs ) def predict_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Prediction (inference) step. Note: Loss is not calculated for this loop. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () y_trues , y_probs = [], [] # Iterate over batches with torch . inference_mode (): for i , batch in enumerate ( dataloader ): # Forward pass w/ inputs batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return np . vstack ( y_trues ), np . vstack ( y_probs ) def train ( self , num_epochs : int , patience : int , train_dataloader : torch . utils . data . DataLoader , val_dataloader : torch . utils . data . DataLoader , ) -> Tuple : \"\"\"Training loop. Args: num_epochs (int): Maximum number of epochs to train for (can stop earlier based on performance). patience (int): Number of acceptable epochs for continuous degrading performance. train_dataloader (torch.utils.data.DataLoader): Dataloader object with training data split. val_dataloader (torch.utils.data.DataLoader): Dataloader object with validation data split. Raises: optuna.TrialPruned: Early stopping of the optimization trial if poor performance. Returns: The best validation loss and the trained model from that point. \"\"\" best_val_loss = np . inf best_model = None _patience = patience for epoch in range ( num_epochs ): # Steps train_loss = self . train_step ( dataloader = train_dataloader ) val_loss , _ , _ = self . eval_step ( dataloader = val_dataloader ) self . scheduler . step ( val_loss ) # Pruning based on the intermediate value if self . trial : self . trial . report ( val_loss , epoch ) if self . trial . should_prune (): # pragma: no cover, optuna pruning logger . info ( \"Unpromising trial pruned!\" ) raise optuna . TrialPruned () # Early stopping if val_loss < best_val_loss : best_val_loss = val_loss best_model = self . model _patience = patience # reset _patience else : # pragma: no cover, simple subtraction _patience -= 1 if not _patience : # pragma: no cover, simple break logger . info ( \"Stopping early!\" ) break # Logging logger . info ( f \"Epoch: { epoch + 1 } | \" f \"train_loss: { train_loss : .5f } , \" f \"val_loss: { val_loss : .5f } , \" f \"lr: { self . optimizer . param_groups [ 0 ][ 'lr' ] : .2E } , \" f \"_patience: { _patience } \" ) return best_val_loss , best_model eval_step ( self , dataloader ) Evaluation (val / test) step. Parameters: Name Type Description Default dataloader torch.utils.data.DataLoader Torch dataloader to load batches from. required Source code in mlops_with_ml/train.py def eval_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Evaluation (val / test) step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () loss = 0.0 y_trues , y_probs = [], [] # Iterate over val batches with torch . inference_mode (): for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , y_true ) . item () # Cumulative Metrics loss += ( J - loss ) / ( i + 1 ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return loss , np . vstack ( y_trues ), np . vstack ( y_probs ) predict_step ( self , dataloader ) Note Loss is not calculated for this loop. Parameters: Name Type Description Default dataloader torch.utils.data.DataLoader Torch dataloader to load batches from. required Source code in mlops_with_ml/train.py def predict_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Prediction (inference) step. Note: Loss is not calculated for this loop. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () y_trues , y_probs = [], [] # Iterate over batches with torch . inference_mode (): for i , batch in enumerate ( dataloader ): # Forward pass w/ inputs batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return np . vstack ( y_trues ), np . vstack ( y_probs ) train ( self , num_epochs , patience , train_dataloader , val_dataloader ) Training loop. Parameters: Name Type Description Default num_epochs int Maximum number of epochs to train for (can stop earlier based on performance). required patience int Number of acceptable epochs for continuous degrading performance. required train_dataloader torch.utils.data.DataLoader Dataloader object with training data split. required val_dataloader torch.utils.data.DataLoader Dataloader object with validation data split. required Exceptions: Type Description optuna.TrialPruned Early stopping of the optimization trial if poor performance. Returns: Type Description Tuple The best validation loss and the trained model from that point. Source code in mlops_with_ml/train.py def train ( self , num_epochs : int , patience : int , train_dataloader : torch . utils . data . DataLoader , val_dataloader : torch . utils . data . DataLoader , ) -> Tuple : \"\"\"Training loop. Args: num_epochs (int): Maximum number of epochs to train for (can stop earlier based on performance). patience (int): Number of acceptable epochs for continuous degrading performance. train_dataloader (torch.utils.data.DataLoader): Dataloader object with training data split. val_dataloader (torch.utils.data.DataLoader): Dataloader object with validation data split. Raises: optuna.TrialPruned: Early stopping of the optimization trial if poor performance. Returns: The best validation loss and the trained model from that point. \"\"\" best_val_loss = np . inf best_model = None _patience = patience for epoch in range ( num_epochs ): # Steps train_loss = self . train_step ( dataloader = train_dataloader ) val_loss , _ , _ = self . eval_step ( dataloader = val_dataloader ) self . scheduler . step ( val_loss ) # Pruning based on the intermediate value if self . trial : self . trial . report ( val_loss , epoch ) if self . trial . should_prune (): # pragma: no cover, optuna pruning logger . info ( \"Unpromising trial pruned!\" ) raise optuna . TrialPruned () # Early stopping if val_loss < best_val_loss : best_val_loss = val_loss best_model = self . model _patience = patience # reset _patience else : # pragma: no cover, simple subtraction _patience -= 1 if not _patience : # pragma: no cover, simple break logger . info ( \"Stopping early!\" ) break # Logging logger . info ( f \"Epoch: { epoch + 1 } | \" f \"train_loss: { train_loss : .5f } , \" f \"val_loss: { val_loss : .5f } , \" f \"lr: { self . optimizer . param_groups [ 0 ][ 'lr' ] : .2E } , \" f \"_patience: { _patience } \" ) return best_val_loss , best_model train_step ( self , dataloader ) Train step. Parameters: Name Type Description Default dataloader torch.utils.data.DataLoader Torch dataloader to load batches from. required Source code in mlops_with_ml/train.py def train_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Train step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to train mode self . model . train () loss = 0.0 # Iterate over train batches for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , targets = batch [: - 1 ], batch [ - 1 ] self . optimizer . zero_grad () # Reset gradients z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , targets ) # Define loss J . backward () # Backward pass self . optimizer . step () # Update weights # Cumulative Metrics loss += ( J . detach () . item () - loss ) / ( i + 1 ) return loss find_best_threshold ( y_true , y_prob ) Determine the best threshold for maximum f1 score. Usage: # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) params . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) Parameters: Name Type Description Default y_true np.ndarray True labels. required y_prob np.ndarray Probability distribution for predicted labels. required Returns: Type Description float Best threshold for maximum f1 score. Source code in mlops_with_ml/train.py def find_best_threshold ( y_true : np . ndarray , y_prob : np . ndarray ) -> float : \"\"\"Determine the best threshold for maximum f1 score. Usage: ```python # Find best threshold _, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader) params.threshold = find_best_threshold(y_true=y_true, y_prob=y_prob) ``` Args: y_true (np.ndarray): True labels. y_prob (np.ndarray): Probability distribution for predicted labels. Returns: Best threshold for maximum f1 score. \"\"\" precisions , recalls , thresholds = precision_recall_curve ( y_true . ravel (), y_prob . ravel () ) f1s = ( 2 * precisions * recalls ) / ( precisions + recalls ) return thresholds [ np . argmax ( f1s )] objective ( params , trial ) Objective function for optimization trials. Parameters: Name Type Description Default params Namespace Input parameters for each trial (see config/params.json ). required trial optuna.trial._trial.Trial Optuna optimization trial. required Returns: Type Description float F1 score from evaluating the trained model on the test data split. Source code in mlops_with_ml/train.py def objective ( params : Namespace , trial : optuna . trial . _trial . Trial ) -> float : \"\"\"Objective function for optimization trials. Args: params (Namespace): Input parameters for each trial (see `config/params.json`). trial (optuna.trial._trial.Trial): Optuna optimization trial. Returns: F1 score from evaluating the trained model on the test data split. \"\"\" # Paramters (to tune) params . embedding_dim = trial . suggest_int ( \"embedding_dim\" , 128 , 512 ) params . num_filters = trial . suggest_int ( \"num_filters\" , 128 , 512 ) params . hidden_dim = trial . suggest_int ( \"hidden_dim\" , 128 , 512 ) params . dropout_p = trial . suggest_uniform ( \"dropout_p\" , 0.3 , 0.8 ) params . lr = trial . suggest_loguniform ( \"lr\" , 5e-5 , 5e-4 ) # Train (can move some of these outside for efficiency) logger . info ( f \" \\n Trial { trial . number } :\" ) logger . info ( json . dumps ( trial . params , indent = 2 )) artifacts = train ( params = params , trial = trial ) # Set additional attributes params = artifacts [ \"params\" ] performance = artifacts [ \"performance\" ] logger . info ( json . dumps ( performance [ \"overall\" ], indent = 2 )) trial . set_user_attr ( \"threshold\" , params . threshold ) trial . set_user_attr ( \"precision\" , performance [ \"overall\" ][ \"precision\" ]) trial . set_user_attr ( \"recall\" , performance [ \"overall\" ][ \"recall\" ]) trial . set_user_attr ( \"f1\" , performance [ \"overall\" ][ \"f1\" ]) return performance [ \"overall\" ][ \"f1\" ] train ( params , trial = None ) Operations for training. Parameters: Name Type Description Default params Namespace Input parameters for operations. required trial optuna.trial._trial.Trail Optuna optimization trial. Defaults to None. None Returns: Type Description Dict Artifacts to save and load for later. Source code in mlops_with_ml/train.py def train ( params : Namespace , trial : optuna . trial . _trial . Trial = None ) -> Dict : \"\"\"Operations for training. Args: params (Namespace): Input parameters for operations. trial (optuna.trial._trial.Trail, optional): Optuna optimization trial. Defaults to None. Returns: Artifacts to save and load for later. \"\"\" # Set up utils . set_seed ( seed = params . seed ) device = utils . set_device ( cuda = params . cuda ) # Load features features_fp = Path ( config . DATA_RAW_DIR , \"features.json\" ) tags_fp = Path ( config . DATA_RAW_DIR , \"tags.json\" ) features = utils . load_dict ( filepath = features_fp ) tags_dict = utils . list_to_dict ( utils . load_dict ( filepath = tags_fp ), key = \"tag\" ) df = pd . DataFrame ( features ) if params . shuffle : df = df . sample ( frac = 1 ) . reset_index ( drop = True ) df = df [: params . subset ] # None = all samples # Prepare data (filter, clean, etc.) df , tags_above_freq , tags_below_freq = data . prepare ( df = df , include = list ( tags_dict . keys ()), exclude = config . EXCLUDED_TAGS , min_tag_freq = params . min_tag_freq , ) params . num_samples = len ( df ) # Preprocess data df . text = df . text . apply ( data . preprocess , lower = params . lower , stem = params . stem ) # Encode labels labels = df . tags label_encoder = data . MultiLabelLabelEncoder () label_encoder . fit ( labels ) y = label_encoder . encode ( labels ) # Class weights all_tags = list ( itertools . chain . from_iterable ( labels . values )) counts = np . bincount ([ label_encoder . class_to_index [ class_ ] for class_ in all_tags ]) class_weights = { i : 1.0 / count for i , count in enumerate ( counts )} # Split data utils . set_seed ( seed = params . seed ) # needed for skmultilearn X = df . text . to_numpy () X_train , X_ , y_train , y_ = data . iterative_train_test_split ( X = X , y = y , train_size = params . train_size ) X_val , X_test , y_val , y_test = data . iterative_train_test_split ( X = X_ , y = y_ , train_size = 0.5 ) test_df = pd . DataFrame ({ \"text\" : X_test , \"tags\" : label_encoder . decode ( y_test )}) # Tokenize inputs tokenizer = data . Tokenizer ( char_level = params . char_level ) tokenizer . fit_on_texts ( texts = X_train ) X_train = np . array ( tokenizer . texts_to_sequences ( X_train ), dtype = object ) X_val = np . array ( tokenizer . texts_to_sequences ( X_val ), dtype = object ) X_test = np . array ( tokenizer . texts_to_sequences ( X_test ), dtype = object ) # Create dataloaders train_dataset = data . CNNTextDataset ( X = X_train , y = y_train , max_filter_size = params . max_filter_size ) val_dataset = data . CNNTextDataset ( X = X_val , y = y_val , max_filter_size = params . max_filter_size ) train_dataloader = train_dataset . create_dataloader ( batch_size = params . batch_size ) val_dataloader = val_dataset . create_dataloader ( batch_size = params . batch_size ) # Initialize model model = models . initialize_model ( params = params , vocab_size = len ( tokenizer ), num_classes = len ( label_encoder ), device = device , ) # Train model logger . info ( f \"Parameters: { json . dumps ( params . __dict__ , indent = 2 , cls = NumpyEncoder ) } \" ) class_weights_tensor = torch . Tensor ( np . array ( list ( class_weights . values ()))) loss_fn = nn . BCEWithLogitsLoss ( weight = class_weights_tensor ) optimizer = torch . optim . Adam ( model . parameters (), lr = params . lr ) scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , factor = 0.05 , patience = 5 ) # Trainer module trainer = Trainer ( model = model , device = device , loss_fn = loss_fn , optimizer = optimizer , scheduler = scheduler , trial = trial , ) # Train best_val_loss , best_model = trainer . train ( params . num_epochs , params . patience , train_dataloader , val_dataloader ) # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) params . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) # Evaluate model artifacts = { \"params\" : params , \"label_encoder\" : label_encoder , \"tokenizer\" : tokenizer , \"model\" : best_model , \"loss\" : best_val_loss , } device = torch . device ( \"cpu\" ) y_true , y_pred , performance = eval . evaluate ( df = test_df , artifacts = artifacts ) artifacts [ \"performance\" ] = performance return artifacts","title":"Training"},{"location":"mlops_with_ml/train/#mlops_with_ml.train.Trainer","text":"Object used to facilitate training. Source code in mlops_with_ml/train.py class Trainer : \"\"\"Object used to facilitate training.\"\"\" def __init__ ( self , model : nn . Module , device : torch . device = torch . device ( \"cpu\" ), loss_fn = None , optimizer = None , scheduler = None , trial : optuna . trial . _trial . Trial = None , ): # Set params self . model = model self . device = device self . loss_fn = loss_fn self . optimizer = optimizer self . scheduler = scheduler self . trial = trial def train_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Train step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to train mode self . model . train () loss = 0.0 # Iterate over train batches for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , targets = batch [: - 1 ], batch [ - 1 ] self . optimizer . zero_grad () # Reset gradients z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , targets ) # Define loss J . backward () # Backward pass self . optimizer . step () # Update weights # Cumulative Metrics loss += ( J . detach () . item () - loss ) / ( i + 1 ) return loss def eval_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Evaluation (val / test) step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () loss = 0.0 y_trues , y_probs = [], [] # Iterate over val batches with torch . inference_mode (): for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , y_true ) . item () # Cumulative Metrics loss += ( J - loss ) / ( i + 1 ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return loss , np . vstack ( y_trues ), np . vstack ( y_probs ) def predict_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Prediction (inference) step. Note: Loss is not calculated for this loop. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () y_trues , y_probs = [], [] # Iterate over batches with torch . inference_mode (): for i , batch in enumerate ( dataloader ): # Forward pass w/ inputs batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return np . vstack ( y_trues ), np . vstack ( y_probs ) def train ( self , num_epochs : int , patience : int , train_dataloader : torch . utils . data . DataLoader , val_dataloader : torch . utils . data . DataLoader , ) -> Tuple : \"\"\"Training loop. Args: num_epochs (int): Maximum number of epochs to train for (can stop earlier based on performance). patience (int): Number of acceptable epochs for continuous degrading performance. train_dataloader (torch.utils.data.DataLoader): Dataloader object with training data split. val_dataloader (torch.utils.data.DataLoader): Dataloader object with validation data split. Raises: optuna.TrialPruned: Early stopping of the optimization trial if poor performance. Returns: The best validation loss and the trained model from that point. \"\"\" best_val_loss = np . inf best_model = None _patience = patience for epoch in range ( num_epochs ): # Steps train_loss = self . train_step ( dataloader = train_dataloader ) val_loss , _ , _ = self . eval_step ( dataloader = val_dataloader ) self . scheduler . step ( val_loss ) # Pruning based on the intermediate value if self . trial : self . trial . report ( val_loss , epoch ) if self . trial . should_prune (): # pragma: no cover, optuna pruning logger . info ( \"Unpromising trial pruned!\" ) raise optuna . TrialPruned () # Early stopping if val_loss < best_val_loss : best_val_loss = val_loss best_model = self . model _patience = patience # reset _patience else : # pragma: no cover, simple subtraction _patience -= 1 if not _patience : # pragma: no cover, simple break logger . info ( \"Stopping early!\" ) break # Logging logger . info ( f \"Epoch: { epoch + 1 } | \" f \"train_loss: { train_loss : .5f } , \" f \"val_loss: { val_loss : .5f } , \" f \"lr: { self . optimizer . param_groups [ 0 ][ 'lr' ] : .2E } , \" f \"_patience: { _patience } \" ) return best_val_loss , best_model","title":"Trainer"},{"location":"mlops_with_ml/train/#mlops_with_ml.train.Trainer.eval_step","text":"Evaluation (val / test) step. Parameters: Name Type Description Default dataloader torch.utils.data.DataLoader Torch dataloader to load batches from. required Source code in mlops_with_ml/train.py def eval_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Evaluation (val / test) step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () loss = 0.0 y_trues , y_probs = [], [] # Iterate over val batches with torch . inference_mode (): for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , y_true ) . item () # Cumulative Metrics loss += ( J - loss ) / ( i + 1 ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return loss , np . vstack ( y_trues ), np . vstack ( y_probs )","title":"eval_step()"},{"location":"mlops_with_ml/train/#mlops_with_ml.train.Trainer.predict_step","text":"Note Loss is not calculated for this loop. Parameters: Name Type Description Default dataloader torch.utils.data.DataLoader Torch dataloader to load batches from. required Source code in mlops_with_ml/train.py def predict_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Prediction (inference) step. Note: Loss is not calculated for this loop. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to eval mode self . model . eval () y_trues , y_probs = [], [] # Iterate over batches with torch . inference_mode (): for i , batch in enumerate ( dataloader ): # Forward pass w/ inputs batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , y_true = batch [: - 1 ], batch [ - 1 ] z = self . model ( inputs ) # Store outputs y_prob = torch . sigmoid ( z ) . cpu () . numpy () y_probs . extend ( y_prob ) y_trues . extend ( y_true . cpu () . numpy ()) return np . vstack ( y_trues ), np . vstack ( y_probs )","title":"predict_step()"},{"location":"mlops_with_ml/train/#mlops_with_ml.train.Trainer.train","text":"Training loop. Parameters: Name Type Description Default num_epochs int Maximum number of epochs to train for (can stop earlier based on performance). required patience int Number of acceptable epochs for continuous degrading performance. required train_dataloader torch.utils.data.DataLoader Dataloader object with training data split. required val_dataloader torch.utils.data.DataLoader Dataloader object with validation data split. required Exceptions: Type Description optuna.TrialPruned Early stopping of the optimization trial if poor performance. Returns: Type Description Tuple The best validation loss and the trained model from that point. Source code in mlops_with_ml/train.py def train ( self , num_epochs : int , patience : int , train_dataloader : torch . utils . data . DataLoader , val_dataloader : torch . utils . data . DataLoader , ) -> Tuple : \"\"\"Training loop. Args: num_epochs (int): Maximum number of epochs to train for (can stop earlier based on performance). patience (int): Number of acceptable epochs for continuous degrading performance. train_dataloader (torch.utils.data.DataLoader): Dataloader object with training data split. val_dataloader (torch.utils.data.DataLoader): Dataloader object with validation data split. Raises: optuna.TrialPruned: Early stopping of the optimization trial if poor performance. Returns: The best validation loss and the trained model from that point. \"\"\" best_val_loss = np . inf best_model = None _patience = patience for epoch in range ( num_epochs ): # Steps train_loss = self . train_step ( dataloader = train_dataloader ) val_loss , _ , _ = self . eval_step ( dataloader = val_dataloader ) self . scheduler . step ( val_loss ) # Pruning based on the intermediate value if self . trial : self . trial . report ( val_loss , epoch ) if self . trial . should_prune (): # pragma: no cover, optuna pruning logger . info ( \"Unpromising trial pruned!\" ) raise optuna . TrialPruned () # Early stopping if val_loss < best_val_loss : best_val_loss = val_loss best_model = self . model _patience = patience # reset _patience else : # pragma: no cover, simple subtraction _patience -= 1 if not _patience : # pragma: no cover, simple break logger . info ( \"Stopping early!\" ) break # Logging logger . info ( f \"Epoch: { epoch + 1 } | \" f \"train_loss: { train_loss : .5f } , \" f \"val_loss: { val_loss : .5f } , \" f \"lr: { self . optimizer . param_groups [ 0 ][ 'lr' ] : .2E } , \" f \"_patience: { _patience } \" ) return best_val_loss , best_model","title":"train()"},{"location":"mlops_with_ml/train/#mlops_with_ml.train.Trainer.train_step","text":"Train step. Parameters: Name Type Description Default dataloader torch.utils.data.DataLoader Torch dataloader to load batches from. required Source code in mlops_with_ml/train.py def train_step ( self , dataloader : torch . utils . data . DataLoader ): \"\"\"Train step. Args: dataloader (torch.utils.data.DataLoader): Torch dataloader to load batches from. \"\"\" # Set model to train mode self . model . train () loss = 0.0 # Iterate over train batches for i , batch in enumerate ( dataloader ): # Step batch = [ item . to ( self . device ) for item in batch ] # Set device inputs , targets = batch [: - 1 ], batch [ - 1 ] self . optimizer . zero_grad () # Reset gradients z = self . model ( inputs ) # Forward pass J = self . loss_fn ( z , targets ) # Define loss J . backward () # Backward pass self . optimizer . step () # Update weights # Cumulative Metrics loss += ( J . detach () . item () - loss ) / ( i + 1 ) return loss","title":"train_step()"},{"location":"mlops_with_ml/train/#mlops_with_ml.train.find_best_threshold","text":"Determine the best threshold for maximum f1 score. Usage: # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) params . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) Parameters: Name Type Description Default y_true np.ndarray True labels. required y_prob np.ndarray Probability distribution for predicted labels. required Returns: Type Description float Best threshold for maximum f1 score. Source code in mlops_with_ml/train.py def find_best_threshold ( y_true : np . ndarray , y_prob : np . ndarray ) -> float : \"\"\"Determine the best threshold for maximum f1 score. Usage: ```python # Find best threshold _, y_true, y_prob = trainer.eval_step(dataloader=train_dataloader) params.threshold = find_best_threshold(y_true=y_true, y_prob=y_prob) ``` Args: y_true (np.ndarray): True labels. y_prob (np.ndarray): Probability distribution for predicted labels. Returns: Best threshold for maximum f1 score. \"\"\" precisions , recalls , thresholds = precision_recall_curve ( y_true . ravel (), y_prob . ravel () ) f1s = ( 2 * precisions * recalls ) / ( precisions + recalls ) return thresholds [ np . argmax ( f1s )]","title":"find_best_threshold()"},{"location":"mlops_with_ml/train/#mlops_with_ml.train.objective","text":"Objective function for optimization trials. Parameters: Name Type Description Default params Namespace Input parameters for each trial (see config/params.json ). required trial optuna.trial._trial.Trial Optuna optimization trial. required Returns: Type Description float F1 score from evaluating the trained model on the test data split. Source code in mlops_with_ml/train.py def objective ( params : Namespace , trial : optuna . trial . _trial . Trial ) -> float : \"\"\"Objective function for optimization trials. Args: params (Namespace): Input parameters for each trial (see `config/params.json`). trial (optuna.trial._trial.Trial): Optuna optimization trial. Returns: F1 score from evaluating the trained model on the test data split. \"\"\" # Paramters (to tune) params . embedding_dim = trial . suggest_int ( \"embedding_dim\" , 128 , 512 ) params . num_filters = trial . suggest_int ( \"num_filters\" , 128 , 512 ) params . hidden_dim = trial . suggest_int ( \"hidden_dim\" , 128 , 512 ) params . dropout_p = trial . suggest_uniform ( \"dropout_p\" , 0.3 , 0.8 ) params . lr = trial . suggest_loguniform ( \"lr\" , 5e-5 , 5e-4 ) # Train (can move some of these outside for efficiency) logger . info ( f \" \\n Trial { trial . number } :\" ) logger . info ( json . dumps ( trial . params , indent = 2 )) artifacts = train ( params = params , trial = trial ) # Set additional attributes params = artifacts [ \"params\" ] performance = artifacts [ \"performance\" ] logger . info ( json . dumps ( performance [ \"overall\" ], indent = 2 )) trial . set_user_attr ( \"threshold\" , params . threshold ) trial . set_user_attr ( \"precision\" , performance [ \"overall\" ][ \"precision\" ]) trial . set_user_attr ( \"recall\" , performance [ \"overall\" ][ \"recall\" ]) trial . set_user_attr ( \"f1\" , performance [ \"overall\" ][ \"f1\" ]) return performance [ \"overall\" ][ \"f1\" ]","title":"objective()"},{"location":"mlops_with_ml/train/#mlops_with_ml.train.train","text":"Operations for training. Parameters: Name Type Description Default params Namespace Input parameters for operations. required trial optuna.trial._trial.Trail Optuna optimization trial. Defaults to None. None Returns: Type Description Dict Artifacts to save and load for later. Source code in mlops_with_ml/train.py def train ( params : Namespace , trial : optuna . trial . _trial . Trial = None ) -> Dict : \"\"\"Operations for training. Args: params (Namespace): Input parameters for operations. trial (optuna.trial._trial.Trail, optional): Optuna optimization trial. Defaults to None. Returns: Artifacts to save and load for later. \"\"\" # Set up utils . set_seed ( seed = params . seed ) device = utils . set_device ( cuda = params . cuda ) # Load features features_fp = Path ( config . DATA_RAW_DIR , \"features.json\" ) tags_fp = Path ( config . DATA_RAW_DIR , \"tags.json\" ) features = utils . load_dict ( filepath = features_fp ) tags_dict = utils . list_to_dict ( utils . load_dict ( filepath = tags_fp ), key = \"tag\" ) df = pd . DataFrame ( features ) if params . shuffle : df = df . sample ( frac = 1 ) . reset_index ( drop = True ) df = df [: params . subset ] # None = all samples # Prepare data (filter, clean, etc.) df , tags_above_freq , tags_below_freq = data . prepare ( df = df , include = list ( tags_dict . keys ()), exclude = config . EXCLUDED_TAGS , min_tag_freq = params . min_tag_freq , ) params . num_samples = len ( df ) # Preprocess data df . text = df . text . apply ( data . preprocess , lower = params . lower , stem = params . stem ) # Encode labels labels = df . tags label_encoder = data . MultiLabelLabelEncoder () label_encoder . fit ( labels ) y = label_encoder . encode ( labels ) # Class weights all_tags = list ( itertools . chain . from_iterable ( labels . values )) counts = np . bincount ([ label_encoder . class_to_index [ class_ ] for class_ in all_tags ]) class_weights = { i : 1.0 / count for i , count in enumerate ( counts )} # Split data utils . set_seed ( seed = params . seed ) # needed for skmultilearn X = df . text . to_numpy () X_train , X_ , y_train , y_ = data . iterative_train_test_split ( X = X , y = y , train_size = params . train_size ) X_val , X_test , y_val , y_test = data . iterative_train_test_split ( X = X_ , y = y_ , train_size = 0.5 ) test_df = pd . DataFrame ({ \"text\" : X_test , \"tags\" : label_encoder . decode ( y_test )}) # Tokenize inputs tokenizer = data . Tokenizer ( char_level = params . char_level ) tokenizer . fit_on_texts ( texts = X_train ) X_train = np . array ( tokenizer . texts_to_sequences ( X_train ), dtype = object ) X_val = np . array ( tokenizer . texts_to_sequences ( X_val ), dtype = object ) X_test = np . array ( tokenizer . texts_to_sequences ( X_test ), dtype = object ) # Create dataloaders train_dataset = data . CNNTextDataset ( X = X_train , y = y_train , max_filter_size = params . max_filter_size ) val_dataset = data . CNNTextDataset ( X = X_val , y = y_val , max_filter_size = params . max_filter_size ) train_dataloader = train_dataset . create_dataloader ( batch_size = params . batch_size ) val_dataloader = val_dataset . create_dataloader ( batch_size = params . batch_size ) # Initialize model model = models . initialize_model ( params = params , vocab_size = len ( tokenizer ), num_classes = len ( label_encoder ), device = device , ) # Train model logger . info ( f \"Parameters: { json . dumps ( params . __dict__ , indent = 2 , cls = NumpyEncoder ) } \" ) class_weights_tensor = torch . Tensor ( np . array ( list ( class_weights . values ()))) loss_fn = nn . BCEWithLogitsLoss ( weight = class_weights_tensor ) optimizer = torch . optim . Adam ( model . parameters (), lr = params . lr ) scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , factor = 0.05 , patience = 5 ) # Trainer module trainer = Trainer ( model = model , device = device , loss_fn = loss_fn , optimizer = optimizer , scheduler = scheduler , trial = trial , ) # Train best_val_loss , best_model = trainer . train ( params . num_epochs , params . patience , train_dataloader , val_dataloader ) # Find best threshold _ , y_true , y_prob = trainer . eval_step ( dataloader = train_dataloader ) params . threshold = find_best_threshold ( y_true = y_true , y_prob = y_prob ) # Evaluate model artifacts = { \"params\" : params , \"label_encoder\" : label_encoder , \"tokenizer\" : tokenizer , \"model\" : best_model , \"loss\" : best_val_loss , } device = torch . device ( \"cpu\" ) y_true , y_pred , performance = eval . evaluate ( df = test_df , artifacts = artifacts ) artifacts [ \"performance\" ] = performance return artifacts","title":"train()"},{"location":"mlops_with_ml/utils/","text":"dict_diff ( d_a , d_b , d_a_name = 'a' , d_b_name = 'b' ) Differences between two dictionaries with numerical values. Parameters: Name Type Description Default d_a Dict Dictionary with data. required d_b Dict Dictionary to compare to. required d_a_name str Name of dict a. 'a' d_b_name str Name of dict b. 'b' Returns: Type Description Dict Differences between keys with numerical values. Source code in mlops_with_ml/utils.py def dict_diff ( d_a : Dict , d_b : Dict , d_a_name = \"a\" , d_b_name = \"b\" ) -> Dict : \"\"\"Differences between two dictionaries with numerical values. Args: d_a (Dict): Dictionary with data. d_b (Dict): Dictionary to compare to. d_a_name (str): Name of dict a. d_b_name (str): Name of dict b. Returns: Dict: Differences between keys with numerical values. \"\"\" # Recursively flatten d_a = pd . json_normalize ( d_a , sep = \".\" ) . to_dict ( orient = \"records\" )[ 0 ] d_b = pd . json_normalize ( d_b , sep = \".\" ) . to_dict ( orient = \"records\" )[ 0 ] if d_a . keys () != d_b . keys (): raise Exception ( \"Cannot compare these dictionaries because they have different keys.\" ) # Compare diff = {} for key in d_a : if isinstance ( d_a [ key ], numbers . Number ) and isinstance ( d_b [ key ], numbers . Number ): diff [ key ] = { d_a_name : d_a [ key ], d_b_name : d_b [ key ], \"diff\" : d_a [ key ] - d_b [ key ], } return diff list_to_dict ( list_of_dicts , key ) Convert a list of dict_a to a dict_b where the key in dict_b is an item in each dict_a . Parameters: Name Type Description Default list_of_dicts List list of items to convert to dict. required key str Name of the item in dict_a to use as primary key for dict_b . required Returns: Type Description Dict A dictionary with items from the list organized by key. Source code in mlops_with_ml/utils.py def list_to_dict ( list_of_dicts : List , key : str ) -> Dict : \"\"\"Convert a list of `dict_a` to a `dict_b` where the `key` in `dict_b` is an item in each `dict_a`. Args: list_of_dicts (List): list of items to convert to dict. key (str): Name of the item in `dict_a` to use as primary key for `dict_b`. Returns: A dictionary with items from the list organized by key. \"\"\" d_b = {} for d_a in list_of_dicts : d_b_key = d_a . pop ( key ) d_b [ d_b_key ] = d_a return d_b load_dict ( filepath ) Load a dictionary from a JSON's filepath. Parameters: Name Type Description Default filepath str JSON's filepath. required Returns: Type Description Dict A dictionary with the data loaded. Source code in mlops_with_ml/utils.py def load_dict ( filepath : str ) -> Dict : \"\"\"Load a dictionary from a JSON's filepath. Args: filepath (str): JSON's filepath. Returns: A dictionary with the data loaded. \"\"\" with open ( filepath ) as fp : d = json . load ( fp ) return d load_json_from_url ( url ) Load JSON data from a URL. Parameters: Name Type Description Default url str URL of the data source. required Returns: Type Description Dict A dictionary with the loaded JSON data. Source code in mlops_with_ml/utils.py def load_json_from_url ( url : str ) -> Dict : \"\"\"Load JSON data from a URL. Args: url (str): URL of the data source. Returns: A dictionary with the loaded JSON data. \"\"\" data = json . loads ( urlopen ( url ) . read ()) return data save_dict ( d , filepath , cls = None , sortkeys = False ) Warning This will overwrite any existing file at filepath . Parameters: Name Type Description Default d Dict dictionary to save. required filepath str location to save the dictionary to as a JSON file. required cls optional encoder to use on dict data. Defaults to None. None sortkeys bool sort keys in dict alphabetically. Defaults to False. False Source code in mlops_with_ml/utils.py def save_dict ( d : Dict , filepath : str , cls = None , sortkeys : bool = False ) -> None : \"\"\"Save a dictionary to a specific location. Warning: This will overwrite any existing file at `filepath`. Args: d (Dict): dictionary to save. filepath (str): location to save the dictionary to as a JSON file. cls (optional): encoder to use on dict data. Defaults to None. sortkeys (bool, optional): sort keys in dict alphabetically. Defaults to False. \"\"\" with open ( filepath , \"w\" ) as fp : json . dump ( d , indent = 2 , fp = fp , cls = cls , sort_keys = sortkeys ) set_device ( cuda ) Set the device for computation. Parameters: Name Type Description Default cuda bool Determine whether to use GPU or not (if available). required Returns: Type Description device Device that will be use for compute. Source code in mlops_with_ml/utils.py def set_device ( cuda : bool ) -> torch . device : \"\"\"Set the device for computation. Args: cuda (bool): Determine whether to use GPU or not (if available). Returns: Device that will be use for compute. \"\"\" device = torch . device ( \"cuda\" if ( torch . cuda . is_available () and cuda ) else \"cpu\" ) torch . set_default_tensor_type ( \"torch.FloatTensor\" ) if device . type == \"cuda\" : # pragma: no cover, simple tensor type setting torch . set_default_tensor_type ( \"torch.cuda.FloatTensor\" ) return device set_seed ( seed = 1234 ) Set seed for reproducibility. Parameters: Name Type Description Default seed int number to use as the seed. Defaults to 1234. 1234 Source code in mlops_with_ml/utils.py def set_seed ( seed : int = 1234 ) -> None : \"\"\"Set seed for reproducibility. Args: seed (int, optional): number to use as the seed. Defaults to 1234. \"\"\" # Set seeds np . random . seed ( seed ) random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) # multi-GPU","title":"Utilities"},{"location":"mlops_with_ml/utils/#mlops_with_ml.utils.dict_diff","text":"Differences between two dictionaries with numerical values. Parameters: Name Type Description Default d_a Dict Dictionary with data. required d_b Dict Dictionary to compare to. required d_a_name str Name of dict a. 'a' d_b_name str Name of dict b. 'b' Returns: Type Description Dict Differences between keys with numerical values. Source code in mlops_with_ml/utils.py def dict_diff ( d_a : Dict , d_b : Dict , d_a_name = \"a\" , d_b_name = \"b\" ) -> Dict : \"\"\"Differences between two dictionaries with numerical values. Args: d_a (Dict): Dictionary with data. d_b (Dict): Dictionary to compare to. d_a_name (str): Name of dict a. d_b_name (str): Name of dict b. Returns: Dict: Differences between keys with numerical values. \"\"\" # Recursively flatten d_a = pd . json_normalize ( d_a , sep = \".\" ) . to_dict ( orient = \"records\" )[ 0 ] d_b = pd . json_normalize ( d_b , sep = \".\" ) . to_dict ( orient = \"records\" )[ 0 ] if d_a . keys () != d_b . keys (): raise Exception ( \"Cannot compare these dictionaries because they have different keys.\" ) # Compare diff = {} for key in d_a : if isinstance ( d_a [ key ], numbers . Number ) and isinstance ( d_b [ key ], numbers . Number ): diff [ key ] = { d_a_name : d_a [ key ], d_b_name : d_b [ key ], \"diff\" : d_a [ key ] - d_b [ key ], } return diff","title":"dict_diff()"},{"location":"mlops_with_ml/utils/#mlops_with_ml.utils.list_to_dict","text":"Convert a list of dict_a to a dict_b where the key in dict_b is an item in each dict_a . Parameters: Name Type Description Default list_of_dicts List list of items to convert to dict. required key str Name of the item in dict_a to use as primary key for dict_b . required Returns: Type Description Dict A dictionary with items from the list organized by key. Source code in mlops_with_ml/utils.py def list_to_dict ( list_of_dicts : List , key : str ) -> Dict : \"\"\"Convert a list of `dict_a` to a `dict_b` where the `key` in `dict_b` is an item in each `dict_a`. Args: list_of_dicts (List): list of items to convert to dict. key (str): Name of the item in `dict_a` to use as primary key for `dict_b`. Returns: A dictionary with items from the list organized by key. \"\"\" d_b = {} for d_a in list_of_dicts : d_b_key = d_a . pop ( key ) d_b [ d_b_key ] = d_a return d_b","title":"list_to_dict()"},{"location":"mlops_with_ml/utils/#mlops_with_ml.utils.load_dict","text":"Load a dictionary from a JSON's filepath. Parameters: Name Type Description Default filepath str JSON's filepath. required Returns: Type Description Dict A dictionary with the data loaded. Source code in mlops_with_ml/utils.py def load_dict ( filepath : str ) -> Dict : \"\"\"Load a dictionary from a JSON's filepath. Args: filepath (str): JSON's filepath. Returns: A dictionary with the data loaded. \"\"\" with open ( filepath ) as fp : d = json . load ( fp ) return d","title":"load_dict()"},{"location":"mlops_with_ml/utils/#mlops_with_ml.utils.load_json_from_url","text":"Load JSON data from a URL. Parameters: Name Type Description Default url str URL of the data source. required Returns: Type Description Dict A dictionary with the loaded JSON data. Source code in mlops_with_ml/utils.py def load_json_from_url ( url : str ) -> Dict : \"\"\"Load JSON data from a URL. Args: url (str): URL of the data source. Returns: A dictionary with the loaded JSON data. \"\"\" data = json . loads ( urlopen ( url ) . read ()) return data","title":"load_json_from_url()"},{"location":"mlops_with_ml/utils/#mlops_with_ml.utils.save_dict","text":"Warning This will overwrite any existing file at filepath . Parameters: Name Type Description Default d Dict dictionary to save. required filepath str location to save the dictionary to as a JSON file. required cls optional encoder to use on dict data. Defaults to None. None sortkeys bool sort keys in dict alphabetically. Defaults to False. False Source code in mlops_with_ml/utils.py def save_dict ( d : Dict , filepath : str , cls = None , sortkeys : bool = False ) -> None : \"\"\"Save a dictionary to a specific location. Warning: This will overwrite any existing file at `filepath`. Args: d (Dict): dictionary to save. filepath (str): location to save the dictionary to as a JSON file. cls (optional): encoder to use on dict data. Defaults to None. sortkeys (bool, optional): sort keys in dict alphabetically. Defaults to False. \"\"\" with open ( filepath , \"w\" ) as fp : json . dump ( d , indent = 2 , fp = fp , cls = cls , sort_keys = sortkeys )","title":"save_dict()"},{"location":"mlops_with_ml/utils/#mlops_with_ml.utils.set_device","text":"Set the device for computation. Parameters: Name Type Description Default cuda bool Determine whether to use GPU or not (if available). required Returns: Type Description device Device that will be use for compute. Source code in mlops_with_ml/utils.py def set_device ( cuda : bool ) -> torch . device : \"\"\"Set the device for computation. Args: cuda (bool): Determine whether to use GPU or not (if available). Returns: Device that will be use for compute. \"\"\" device = torch . device ( \"cuda\" if ( torch . cuda . is_available () and cuda ) else \"cpu\" ) torch . set_default_tensor_type ( \"torch.FloatTensor\" ) if device . type == \"cuda\" : # pragma: no cover, simple tensor type setting torch . set_default_tensor_type ( \"torch.cuda.FloatTensor\" ) return device","title":"set_device()"},{"location":"mlops_with_ml/utils/#mlops_with_ml.utils.set_seed","text":"Set seed for reproducibility. Parameters: Name Type Description Default seed int number to use as the seed. Defaults to 1234. 1234 Source code in mlops_with_ml/utils.py def set_seed ( seed : int = 1234 ) -> None : \"\"\"Set seed for reproducibility. Args: seed (int, optional): number to use as the seed. Defaults to 1234. \"\"\" # Set seeds np . random . seed ( seed ) random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) # multi-GPU","title":"set_seed()"}]}